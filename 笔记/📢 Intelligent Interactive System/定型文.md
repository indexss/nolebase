明天，
把iis的卷子重写一遍，好好重写
探索一下作文的句式。
https://gemini.google.com/app/4783b8adb68d12c8
下午把ML的线上题看一下，如果还有时间就看一半的卷子

定型文：
AI-driven XXX tools are increasingly being used by XXX to A, B and C. These systems promise to improve efficiency and objectivity, but they have also raised significant ethical concerns. I will critically analyze the ethical impact of AI in XXX in the following aspects.

First, the collection and use of candidate data bring privacy issues.
- 除了简历中提到的一些内容，一些细节可能会被捕捉
- 比如视频面试中，一些敏感受保护信息如用户的种族，性别，生活环境将会被收集
- 除此之外，信息的存储与使用也是一大问题。这些敏感信息被收集之后，无法确保只被用于本次招聘。若泄露，则会带来很恐怖的影响。

Second, algorithmic bias poses a threat to (XXX) recruitment decisions and workplace diversity.
- AI是数据驱动的，会继承inherit历史数据集中的偏见。
- 若特定岗位的某一性别，族裔的成功率较低，AI算法可能会拒绝目前的应聘者，即使他本人条件十分符合。这样，工作环境的多样性将会被破坏，某一岗位可能会出现全是同一性别/同一族裔的情况

Besides, AI-based hiring tools could exacerbate existing inequalities in employment.


Furthermore, there is tension between automated decision-making and human supervision.
AI承诺自动化和效率，但这不应完全取代人类的判断。GDPR规定stipulates了人有权。。过度依赖自动化决策可能会忽略个体差异，当出现偏差与错误的时候难以修正，造成严重的后果。人类HR的经验以及对公司文化的判断是AI难以替代的，所以必须保留人工审查以及最终的决策权。

Finally, I would like to propose some possible ways to promote AI fairness and AI ethics.

---

AI-driven recruitment tools are increasingly being used by companies to review resumes, analyze interview videos and making decisions. These systems promise to improve efficiency and objectivity, but they have also raised significant ethical concerns. I will critically analyse the ethical impact of AI in hiring in the following aspects.

First, the collection and use of candidate data being privacy issues. Except for the information provided in the resume, some sensitive protected information may be collected and analyzed by the AI system. Such as, the accent, skin color and background environment during the video interview may be used to analyze your race and financial condition. The collection of these data may raise problems like data leakage, abuse or being used for non-recruitment purpose, violate the privacy rights of candidats.
- 敏感受保护隐私数据会被收集
	- 举例，怎么被收集
- 可能会数据泄漏，滥用，被用于非招聘目的，侵犯隐私权

Besides, AI algorithmic bias poses a threat to recruitment decision and workplace diversity. AI are wished to make consistent evaluation on people with similar qualifications, but, AI is a data-driven science, which means that it may inherit the bias and unfair from the dataset. Facebook used to refused a woman candidate for low success rate of her gender and race, although she is qualified this job. Algorithm may also make unreasonable decisions based on the false correlation of data, which is quite unfair. The likely result is that, some job places may be occupied by a specific race or gender, which will break the diversity of the working places.
- 大多数情况下可以做出一致性的判决，但并不意味着能够mitigate偏见
- 可能继承数据集的unbias
	- Facebook例子
- 种族问题结果，降低工作环境多样性。

Furthermore, there is tension between automated decision-making and human supervision. On the one hand, AI did improve the working efficiency. However, on the other hand, the HR staff may over-dependent on these tools and loss autonomy. We believe that human's experience is hard to be replaced by the AI system at this stage. GDPR stipulates that people have the power to refuse decisions made by fully-automated system, so overall the final decision need to be reviewed by human beings.
- 张力，一方面能够改善效率，另一方面可能会过于依赖。
- 人的经验在目前很难被AI替代，AI难以理解公司的目标以及企业文化
- GDPR规定人有权拒绝全自动系统作出的决策，这就需要人来参与进来

Finally, I would like to propose some possible ways to promote AI fairness in recruitment scenario. For the company, First, HRs need to be added in the pipeline of decision making, human and AI should coorperate to make decisions. Specifically, people could use AI to summarize information, solve some dirty work, but should not rely on the decision made by AI. All of the results need to be reviewed and explainable. Second, use some secure AI technology such as federated learning or privacy difference learning to minimize the probability of data leakage. When it comes to privacy data, try to process it locally instead of upload full data to the server to avoid the risk of data leakage. Third, try to use some unbiased algorithm to make decisions. Choose to use AI trained by unbiased dataset. Try to use some metric like Equal Opportunity to evaluate the fairness of the whole system. For the government, try to make relevant laws to avoid abuse of AI tools and automated decision making system. I believe these suggestions will promote a healthier human-ai cooperation.
- 人在回路
	- 举例，什么意思
- 减少隐私信息的收集，泄漏风险
	- 用联邦学习，差分隐私，总之避免
- 用无偏的AI
	- 首先，用无偏数据集训练的ai
	- 另外，用一些metric比如Equal Opportunity去衡量
	- 要与公司目标对其
- 国家立法
![](assets/7AC4FA92-376F-487C-93E1-AE2E3D82FB0D.webp)
人工智能越来越多的被学生利用在学习，辅助做作业，修改文章等。尽管这些工具承诺了效率以及辅助作用，但也引发了重大的伦理关切以及学术诚信问题。
批判性地分析在学习中使用AI的伦理影响。在您的回答中，讨论：
- 隐私问题
- 算法偏见
- 不平等问题
- 人类与AI的紧张关系
- 为确保AI在教育中公平和合乎伦理的使用而采取的可能的保障措施或监管方法
您的回答应展示清晰的论证、伦理推理，并在适当的情况下提供现实世界的例子


Below is a practical **English essay template** you can memorise and adapt to almost any exam question about the ethical impact of AI (medical, recruitment, social-media, etc.). It keeps the language simple, the structure tight, and fits comfortably into **≈500 words (about 35–40 short sentences)**.

---

## 1 Paragraph-by-paragraph skeleton

| ¶                             | Function                  | What to write (≈ words)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ----------------------------- | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1 Introduction**            | _Hook + Context + Thesis_ | 1) Hook: one punchy sentence (“AI now shapes how we …”). 2) Context: 2–3 sentences naming the field **\[FIELD\]** and its promised benefits. 3) Thesis: _“Yet AI raises concerns about privacy, bias, inequality and human oversight, so safeguards are essential.”_ (80-90 w)<br>1）钩：一个简洁的句子（“AI现在塑造了我们如何…”）。2）背景：2-3句话命名领域**\[FIELD\]**及其承诺的好处。3）论点：“然而，AI引发了关于隐私、偏见、不平等和人类监督的担忧，因此必须有保障。”（80-90字）                                                                                                                                                                                                                                                                   |
| **2 Privacy**                 | _Problem 1 + Example_     | Topic sentence: “First, AI threatens personal privacy.” Explain what data the system collects in **\[FIELD\]** and why consent is unclear. Give **one real case** (e.g. DeepMind received 1.6 million NHS patient records without valid consent). Close with why this erodes trust. (≈100 w) ([theguardian.com](https://www.theguardian.com/technology/2017/jul/03/google-deepmind-16m-patient-royal-free-deal-data-protection-act?utm_source=chatgpt.com "Royal Free breached UK data law in 1.6m patient deal with Google's ..."))<br>主题句：“首先，人工智能威胁个人隐私。”解释系统在**\[FIELD\]**中收集了哪些数据，以及为什么同意不明确。给出**一个真实案例**（例如，DeepMind未经有效同意收到了160万名NHS患者记录）。最后解释为什么这会侵蚀信任。（≈100字） |
| **3 Bias & Inequality**       | _Problem 2 & 3 + Example_ | Topic sentence: “Secondly, biased data can amplify inequality.” Describe how skewed training sets harm certain groups. Insert **one case** (e.g. Amazon’s hiring tool downgraded women). Add one sentence about broader social divide. (≈100 w) ([Reuters](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/?utm_source=chatgpt.com "Insight - Amazon scraps secret AI recruiting tool that showed bias ..."))                                                                                                                                                                       |
| **4 Human–AI tension**        | _Problem 4 + Example_     | Topic sentence: “Moreover, over-reliance on automation creates accountability gaps.” Explain decision fatigue or ‘automation bias.’ Illustrate with the COMPAS risk-score debate in US courts. End by asking who is responsible for errors. (≈100 w) ([ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm?utm_source=chatgpt.com "How We Analyzed the COMPAS Recidivism Algorithm - ProPublica"))                                                                                                                                                                                                                          |
| **5 Safeguards & Conclusion** | _Solutions + Closing_     | Start with “To use AI ethically, several safeguards are vital.” List **three**: data-minimisation + regular bias audits + human-in-the-loop. Name at least one law (GDPR or the 2024 EU AI Act). Finish with a balanced sentence: “With clear rules and shared responsibility, AI can support rather than undermine human goals.” (≈100 w) ([European Commission](https://commission.europa.eu/news/ai-act-enters-force-2024-08-01_en?utm_source=chatgpt.com "AI Act enters into force - European Commission"))                                                                                                                                                        |

---

## 2 Useful linking phrases

- **Adding**: moreover · besides · in addition
- **Contrasting**: however · yet · on the other hand
- **Giving examples**: for instance · one clear case is …
- **Cause → effect**: as a result · therefore
- **Concluding**: in short · to sum up · ultimately
---
## 3 “Plug-and-play” evidence bank

| Issue      | Re-usable case                         | One-line note you can quote                                                                                                                                                                                                                                                                                                                                 |
| ---------- | -------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Privacy    | **DeepMind-NHS Streams** (UK, 2015–17) | UK watchdog ruled the hospital broke data-protection law when it shared 1.6 m patient records with Google DeepMind. ([theguardian.com](https://www.theguardian.com/technology/2017/jul/03/google-deepmind-16m-patient-royal-free-deal-data-protection-act?utm_source=chatgpt.com "Royal Free breached UK data law in 1.6m patient deal with Google's ...")) |
| Bias       | **Amazon recruiting tool** (2014-18)   | Reuters revealed the model penalised CVs that contained the word “women,” so Amazon scrapped it. ([Reuters](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/?utm_source=chatgpt.com "Insight - Amazon scraps secret AI recruiting tool that showed bias ..."))           |
| Bias       | **COMPAS risk scores** (US courts)     | ProPublica found Black defendants were almost twice as likely to be labelled “high risk.” ([ProPublica](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm?utm_source=chatgpt.com "How We Analyzed the COMPAS Recidivism Algorithm - ProPublica"))                                                                          |
| Inequality | **Digital divide**                     | Rural schools or small clinics often lack the bandwidth or funds to deploy AI, widening the gap.                                                                                                                                                                                                                                                            |
| Regulation | **EU AI Act (2024)**                   | Classifies “high-risk” systems and demands strict transparency, bias testing and human oversight. ([European Commission](https://commission.europa.eu/news/ai-act-enters-force-2024-08-01_en?utm_source=chatgpt.com "AI Act enters into force - European Commission"))                                                                                      |


---
## 4 Quick writing checklist

1. **Answer the prompt directly** in the first sentence of each body paragraph.
2. **One example per paragraph is enough**—explain it, don’t just name it.
3. **Use the same four-problem pattern** (privacy → bias/inequality → human tension → regulation) for any field.
4. **Stay inside ≈500 words**: five paragraphs of 90–100 words each.
5. **Proof-read for simple grammar**: subject + verb + object; avoid long clauses.

---

AI now ==reshapes how big companies== recruit new employees. AI-driven tools are used to scan CVs, analyze interview videos and even rank the candidates. Although these tools promises speed and objectivity, they also ==triggers serious ethical questions regarding with privacy and bias.== I will critically analyze these ethical problems and try to ==propose some safeguards ==to improve them.

==First, the collection and use of candidate data is facing privacy issue. ==In addition to the information provided in the resume, the AI tools also collects some passive information. For instance, when AI tools are used to analyze online interview, the accent, color of skins, micro-expressions and even background environment can be captured to reveal the race, financial condition and ethnicity. These informations are treated as sensitive protected datas. Using these data to evaluate a candidate may cause false correlation and making bad decisions. Besides, the harm of these data leakage is quite significant. If these data being sold and used in some non-recruitment purpose activity, the trust between human and AI will be eroded.

Besides, biased AI tools may amplify inequality, and harm the diversity in decision making and working places. AI are wished to make consistent and unbiased evaluation on different candidates with similar qualification, but things did not go as planned. AI is a data driven technology. It will inherit the bias in existing data. A well-known case is Amazon used AI recruitment tools in 2018 and find the successful rate of candidates who include word "women" in their CVs is lower. The direct result of this is the diversity of working place will be ruined, one specific job place may be occupied by one ethnicity or one gender, despite there are a lot of qualified candidates. If we cannot use a fair AI tools, it will definetely amplify the inequality in recruitment process.

==Third, there is a growing tension between automated decision making system and human beings. Recruiters enjoy the time saved when AI process hundreds of CVs in seconds, ranking candidates, but this convenience can encourage "automation bias", which means that people just trust the screen without even asking why. ==The GDPR laws found this problems and gives individual right to demand a human review of decisions made by automated system. However, in fact HR teams may rubber-stamp the decision made by AI and just made up some probable reasons, due to most of the systems are lack of explainability and there's no clear party accountable, HR team, algorithm or the creator of the algorithm.

To use AI ethically in recruitment, several safeguards are vital. For the companies side, first the data collection need to be minimized. Only collect the vital information, and use some privacy-friendly technology such as federated learning or privacy differential learning to process most of the data locally instead of uploading them to the server. Second, human need in the loop. Replace some biased and suspicious components in the system with human beings, and the result need to be audited. These people will be treated as the accountable party of this process, and the explainability of the system could be improved. Third, proactively choose unbiased AI systems. Choose the AI system trained with unbiased dataset, or being post-process to reduce inequality. Some metrics like Opportunity Equality could be introduced to check the AI system. For the government, related laws need to be made to regulate the use of AI. EU AI Act in 2024 is a good start.

Overall, I believe the with the law made and rules followed by the company, the application of AI in recruitment will be regulated to achieve the best goal of improving efficiency and healthy development.