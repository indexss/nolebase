# Week 3 Cooperation & Human Decision Making
## 阿克塞尔罗德的计算机锦标赛
著名的博弈论实验，他邀请来自不同学科的研究者提交计算机程序，以竞争“囚徒困境”博弈的最佳策略。
![已上传的图片](assets/4c7ebb2717b4dd260021359f42986f75_MD5.png)
 **友好策略（Nice Strategies，粉色区域）**
- 这些策略具有 **合作性（不主动背叛）**，并且整体得分很高。
- **“以牙还牙”（Tit for Tat）策略** 由 **Anatol Rapoport** 提交，在比赛中表现最佳。
- **这些策略的共同特征**：
    - **从不先背叛**（永远不会主动欺骗对方）
    - **回报合作**（如果对手合作，它们也会合作）
    - **简单而有效**（它们并不复杂，但表现优越）
**得分情况：**
- 这些策略得分都接近600，意味着它们在比赛中表现非常稳定，且能在合作中获得高分。
---
 **造王者（Kingmakers，黄色区域）**
- 这些策略（**Graaskamp 和 Downing**）的特点是它们可能不会自己赢得比赛，但**它们的策略会影响其他策略的排名**。
- 它们的得分变化较大，表明它们可能采取了**不稳定或更具攻击性的策略**，影响了博弈的平衡。
**得分情况：**
- 这些策略的得分波动很大，比如 **Downing 对不同策略的得分从202到625不等**，表明它们对不同对手采取了不同的策略。
---
 **随机策略（Random）**
- **最后一名（Random）** 是完全随机的策略，得分最低（仅442分），因为**随机策略不能形成稳定的合作**，容易被其他策略剥削。


## Axelrod的第二次实验
- 这次实验是对第一次比赛的后续测试，参赛者已经知道第一轮的结果。目标是测试**改进的策略**能否战胜之前的赢家（如 Tit-for-Tat）。
- 参赛者的程序被**随机匹配**，相互竞争。
- 比赛的回合次数是随机的，平均大约200轮。
	- 原因：
		- 这一设计使得策略无法依赖固定轮数（比如如果知道比赛是100轮，就可以在最后几轮背叛）。
		- **逼近现实**：现实世界中的合作往往**没有明确的终点**，所以策略应该适用于不确定的环境。
结果是Tit-for-Tat 仍然获胜。

## Tit-for-Tat告诉我们什么？
- **要友善！（Be Nice!）** - 先从合作开始。
- **要不容忍！（Be intolerant!）** - 如果对方无故背叛，立即进行背叛——展现惩罚的意愿。
- **要宽容！（Be forgiving!）** - 在对方背叛之后（惩罚一次后）重新合作。

## **Evolution of Cooperation（合作的演化）**

### **内容解析**

- 假设有一组智能体，每个都有**3步记忆**。
- 囚徒困境有**4种可能的结果**，因此有 4×4×4=64 4×4=64 种不同的历史状态。
- 一个策略可以被表示为**64维向量**，其中每个元素对应一个动作（合作或背叛）。
- 进化过程：
    1. 初始时，有**20个个体**，每个采用随机策略。
    2. 这些策略相互博弈**多轮**，计算平均得分。
    3. **淘汰最差的10个策略**。
    4. **从前10个策略中交叉“繁殖”**，生成新的策略。
    5. **重复该过程**，模拟策略的进化。
### **结论**
- 通过不断淘汰低效策略并让高得分策略“繁殖”，可以观察到合作策略的演化。
![](assets/Pasted%20image%2020250207232450.webp)
- **初始阶段（前10代）：平均得分下降**
    - 可能由于最初的策略大多是**随机的**，导致合作较少，整体收益低。
    - 也可能是许多**欺骗性策略**占优，使得个体间的信任崩溃，收益减少。
- **10代后：平均得分开始上升**
    - 说明合作策略开始出现，并在进化过程中**逐渐占据主导**。
    - 由于合作可以提高长期收益，这些策略获得更高得分，并在选择过程中存活下来。
- **25代左右：得分短暂下降**
    - 可能是由于新的、较具欺骗性的策略进入种群，导致合作再次受到挑战。
    - 但是这些策略可能**无法长久存活**，因为如果所有个体都开始背叛，整体收益会下降。
- **25代后：平均得分明显上升并趋于稳定**
    - **合作策略占据主导地位**，群体整体得分上升，表明合作是长期博弈中的优胜策略。
    - 这一趋势符合**“以牙还牙”（Tit-for-Tat）等合作策略的演化优势**。

## **Multi-agent Reinforcement Learning（多智能体强化学习）**

### **内容解析**

- 作为**进化算法**的替代方法，可以使用**强化学习（RL）**来学习合作策略。
- **Sandholm 和 Crites（1996）研究**
    - 证明**强化学习智能体可以学会合作**。
    - 当**强化学习智能体与固定的“以牙还牙”（Tit-for-Tat, T4T）策略对战时，T4T会成为学习到的最优策略**。
    - 当**强化学习智能体彼此对战时，结果更加复杂**，说明博弈环境会影响学习到的策略。
### **结论**
- **强化学习**可以自动发现合作策略，但其表现取决于对手的行为模式。

## **Further Discoveries（进一步发现）**
### **Win-Stay Lose-Shift（WSLS）策略**
- **Nowak & Sigmund (1993)** 提出了 **WSLS（赢留输换）** 策略：
    - **比以牙还牙更具鲁棒性**，能更好地应对对手的**意外背叛**。
    - **能够利用纯合作策略**（如 ALLC：始终合作），在某些情况下表现更优。

如果在 Tit-for-Tat（TFT）对局中发生一次**意外背叛**（误操作），该策略会导致：
- **D（背叛）→ C（合作）** 交替模式，使双方都损失较多分数。
- 这可能导致长期的恶性循环，使得合作关系难以恢复。
![](assets/Pasted%20image%2020250207232709.webp)
- **WSLS 赢留输换策略**：赢是指你单方面背叛或者双合作。
    - 如果上轮获胜（双赢或成功欺骗），**继续执行相同策略**。
    - 如果上轮失败（被欺骗或双方背叛），**改变策略**。
- **优势**
    - **能够恢复合作**，不会陷入持续背叛的循环。
    - **在对战 ALLC（始终合作）时表现更优**，可以在适当时机利用对方。带星号是指错误的换了。

![](assets/Pasted%20image%2020250207232757.webp)

## 如果多轮囚徒困境可以说话
1. **合作需要沟通**
    - **合作（Cooperation）** 受**信号传递（Signaling）** 影响，例如语言、承诺或威胁。
    - 在博弈论中，**沟通可以帮助智能体建立信任、制定策略并维持合作**。
2. **S# 算法**    
    - 由 Crandall 等人提出，**结合强化学习（Reinforcement Learning, RL）** 和 **信号机制**。
    - 能够**与人类和其他算法合作**，并达到与人类相当的合作水平。
3. **适用于多种博弈环境**
    - S# 能够在**两人重复随机博弈**（例如囚徒困境）中实现高效合作。
![](assets/Pasted%20image%2020250207233351.webp)
![](assets/Pasted%20image%2020250207233400.webp)
![](assets/Pasted%20image%2020250207233411.webp)
## 设计一个IIS的在线购物平台
### **1️⃣ 信用评分 & 信誉推荐**

- 计算信誉评分 **(0-100)**，高信誉用户在市场上有更大优势。
- **信誉低的用户会被系统限制**（如预付款要求、提高押金等）。
### **2️⃣ AI 信誉检测**
- 采用**机器学习模型**检测欺诈模式，例如：
    - **异常评分**（短时间内大量负评）。
    - **短时间创建大量虚假账户**。
    - **交易模式异常**（某些账户专门给另一个账户打高分）。
### **3️⃣ 交易智能匹配**
- **AI 依据信誉评分匹配买家和卖家**，优先推荐高信誉交易者进行匹配。
- 如果新用户信誉不够，系统可能会要求 **更安全的交易方式**（如托管支付）。

直接互惠（Direct Reciprocity）和间接互惠
![](assets/Pasted%20image%2020250207233758.webp)
直接互惠：A 帮助 B，B 未来回报 A。
间接互惠：A 帮助 B，B 可能不会直接回报 A，但 C（第三方）会因为 A 的善行而帮助 A。
- 依赖于**声誉（Reputation）** 系统，第三方观察 A 的行为后决定是否合作。

声誉如何促进间接互惠？
![](assets/Pasted%20image%2020250207233902.webp)
- 绿色玩家（Player 1）**不会直接与蓝色玩家（Player 2）互动**，但他有 Player 2 的声誉信息。
- 绿色玩家基于历史记录**将 Player 2 归类为“好（G）”或“坏（B）”**。
- **如果 Player 2 的声誉是 G（Good），Player 1 更有可能合作**。

## 在线市场中的声誉
- 在线市场依赖声誉。
- 通过反馈建立声誉。
- 在线市场比传统市场更容易出现欺诈问题。
- 在线和传统市场的参与者以不同的方式进行沟通。
- 沟通模式是否对市场中的信任程度至关重要存在争议。
- 市场中信息传播方式的差异可能会不同程度地影响信任和合作意愿。

## 买家-卖家决策树
![](assets/Pasted%20image%2020250207234119.webp)
买家不买，那么买家本身有35块钱，卖家35块钱。
如果买家买了，商家发货了，买家拿到货物之后主观认为其价值为50，所以付了50，卖家也得到50
如果买家买了，卖家没发货，那么卖家把买家的钱拿了，变成70

## 三种市场
#### **陌生人市场（Stranger Market）**
个体买家和卖家最多只交易一次，**买家无法获得关于卖家交易历史的信息**。
在这种市场中，**道德风险（moral hazard）极大**，因为**卖家的行为不会影响未来潜在客户的决策**，导致卖家可能存在欺诈的动机。

---
#### **反馈市场（Feedback Market）**
一个**在线反馈系统**会**跟踪卖家在过往交易中的履约情况（如是否发货）**，并向潜在买家提供这些信息。
这类市场**提供了间接互惠（Indirect Reciprocity）机制**，使其更符合**在线市场**的特点，因为买家可以基于卖家的历史表现来决定是否交易。

---
#### **伙伴市场（Partners Market）**
买家和卖家会**反复交易**，**在每一轮都会进行互动**。
这种市场**提供了直接互惠（Direct Reciprocity）机制**，更类似于**传统市场**，因为**长期合作关系可以促使卖家提供更好的服务，以维持客户忠诚度**。

## 信任预测
### **如果“交易历史”是唯一重要的因素**
- 假设买家**只关心卖家的历史记录**，而不在意自己是否与该卖家有过直接交易。
- **那么**：
    - **直接互惠（长期合作的买卖关系）** 和 **间接互惠（基于市场反馈的交易）** **应该对市场表现有相同的影响**。
    - 也就是说，**只要卖家有良好历史记录，买家就愿意交易**，无论他们是否之前有过合作。

### **如果“个人交易经验”更重要**
- 假设买家更信任**自己与卖家的直接交易经验**，而不是市场上的反馈信息。
- **那么**：
    - **直接互惠（长期合作的买卖关系）** 会比 **间接互惠（市场评价系统）** 更有效。
    - 也就是说，**买家会更信任自己长期合作的卖家，而不是仅仅依赖市场上的评价系统**。
- 




