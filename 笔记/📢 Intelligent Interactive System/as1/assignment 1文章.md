# Fairness and Bias in Artificial Intelligence
## Introduction


## Research Questions
1. CV领域bias的表现和来源
    CV系统的公平性已经引起了广泛的注意。已经有很多研究从问题角度出发讨论了CV领域的bias，比如种族偏见，性别偏见以及光照适应性等。随着这些问题被发现，CV系统的bias已经不是孤立的问题，而是系统性问题。考虑到目前大部分的CV系统已经转向Learning based，其本质是从数据中学习模式，所以我们将从数据与模型两个角度出发，分析其表现和来源。
    [CV领域bias表现和来源](CV领域bias表现和来源.md)
  1. 简单谈谈bias定义
  2. 来自于数据的bias
  	1. 社会本身就有的bias被收录入了数据集
  	2. 数据采集过程中的缺陷
  	3. 人口学偏差和非人口学
  3. 来自于模型本身缺陷的bias
  	1. 学习过程中学习到了伪相关和固有相关
  	2. 模型设计与训练策略的放大或者弱化
  	3. 人口学与非人口学
1. CV领域de-bias的算法以及其他de-bias的努力
  1. 之前
  	1. 提到一些公平数据集
  	2. 数据去偏
  	3. 数据增强
  2. 之中
  	1. 对抗表示学习
  	2. 正则化/不变性学习
  3. 之后
  	1. 决策阈值或分数校准，后处理层面减轻群体差异


## Research Questions
1. CV领域bias的表现和来源
   - Bias的定义
   - Bias的分类
     - 人口学偏差（Demographic Bias）
     - 非人口学偏差（Non-Demographic Bias）
   - 数据来源的bias
     - 社会固有bias被引入数据集
     - 数据采集过程的问题
     - 任务设计或标注者的主观性
   - 模型本身的bias
     - 伪相关性与固有相关性
     - 训练策略与架构导致的偏差
     - 参数分布对不同群体的影响

2. CV领域de-bias的算法以及其他de-bias的努力
   - 数据层面的去偏（Pre-processing）
     - 公平数据集
     - 数据去偏（Re-sampling, Re-weighting）
     - 数据增强（GAN生成补充样本、多样性数据增强）
   - 模型层面的去偏（In-processing）
     - 对抗表示学习（Adversarial Representation Learning）
     - 正则化/不变性学习
   - 推理层面的去偏（Post-processing）
     - 决策阈值调整
     - 后处理校准

3. 公平性评测标准（Evaluation Metrics）
	[CV领域公平的定义](../CV领域公平的定义.md)
   - 统计公平性（Statistical Parity）
   - 机会公平性（Equalized Odds, Equal Opportunity）
   - Worst-group Accuracy


1. VLM领域bias的表现和来源
	-  Bias的定义，分类
     - 人口学偏差（Demographic Bias）
     - 非人口学偏差（Non-Demographic Bias）
   - 数据来源的bias
     - 社会固有bias被引入数据集
     - 数据采集过程的问题
     - 任务设计或标注者的主观性
   - 模型本身的bias
     - 伪相关性与固有相关性
     - 训练策略与架构导致的偏差
     - 参数分布对不同群体的影响

## Search Strategy
本综述对计算机视觉领域的偏见和公平性研究进行调查。我在多个文献数据库中搜索了相关文献，包括ACM Digital Library, Google Scholar以及arXiv。在检索过程中，我使用了两组并列的关键字，第一组搜索词为：computer vision, CV, image recognition, object detection, semantic segmentation, pose estimation。这些关键词涵盖了计算机视觉的主要任务和方向，确保了搜索结果集中于这一领域。第二组搜索词为：bias, fairness, discrimination, dataset bias, model bias, algorithmic discrimination。这些关键词用于筛选涉及偏见，公平性的研究。

接下来，我对筛选出的文献进行了人工筛选，排除了发布超过5年的文献以及仅仅泛泛而谈并未明确涉及计算机视觉的文献。最终筛选出的工作涵盖了计算机视觉领域的核心偏见问题，包括但不限于计算机视觉领域的数据集偏见，模型偏见，公平性评估指标以及去偏见方法。这一筛选策略确保了研究的广泛性和针对性，同时避免了与其他机器学习领域的公平性研究混淆。

本综述对计算机视觉领域的偏见和公平性研究进行调查。我在多个文献数据库中搜索了相关文献，包括 ACM Digital Library、Google Scholar 以及 arXiv。在检索过程中，我使用了两组并列的关键字，第一组搜索词为：computer vision, CV, image recognition, object detection, semantic segmentation, pose estimation。这些关键词涵盖了计算机视觉的主要任务和方向，确保了搜索结果集中于这一领域。第二组搜索词为：bias, fairness, discrimination, dataset bias, model bias, algorithmic discrimination。这些关键词用于筛选涉及偏见、公平性的研究。 

在筛选过程中，我主要关注近5年内（2019-2024）发表的文献，以保证研究的时效性。同时，我排除了泛泛讨论人工智能公平性但未具体涉及计算机视觉的文献。然而，为了提供完整的背景信息，本综述在介绍计算机视觉偏见的定义、早期研究和经典理论时，也适当引用了5年以上的奠基文献（如最早提出计算机视觉领域偏见的研究）。这一策略确保了研究的广泛性和针对性，同时避免了与其他机器学习领域的公平性研究混淆。

## Summary of Research
## Conclusions
## References




## 一、可研究的研究问题（Research Questions）

1. **RQ1：视觉-语言预训练模型中最常见的社会偏见类型有哪些？它们在不同数据集或应用场景（如图文检索、图像生成、视觉问答）中如何表现？**
   
    - **动机**：了解在 VLM 中出现的性别/种族/年龄等偏见具体体现形式，并梳理这些偏见在不同下游任务中的影响。
    - **意义**：帮助读者快速掌握主要的“痛点”与常见失误模式；为后续提出有针对性的缓解方法奠定基础。
2. **RQ2：目前主流的偏见测量及缓解方法（例如对抗训练、数据增广或后处理技术）在 VLM 中的有效性和局限性是什么？**
   
    - **动机**：概览现有去偏技术在视觉-语言多模态场景下的研究现状、评估指标与结果。
    - **意义**：识别已有方法的优点和不足，为后续改进提供思路，并帮助实践者更好地挑选与应用合适的技术。
3. （可选）**RQ3：在面对多种敏感属性（性别、种族、年龄、肤色等）并存的真实场景时，如何对 VLM 进行多属性联合去偏，以在不显著损害主任务性能的前提下最大化公平性？**
   
    - **动机**：真实应用通常涉及多种敏感属性的同时存在，单一属性去偏不足以应对复杂场景。
    - **意义**：为构建更全面、更具实用性的多模态公平解决方案指明方向，并揭示多属性间可能的交互效应。

可根据实际文献数量、研究深度，选取 1-3 个问题作为论文聚焦点，或者将 RQ2 与 RQ3 合并。

---

## 二、写作大纲（Paper Outline）

以下大纲结合了作业要求所规定的必备章节，并在“文献综述（Summary of Research）”部分细分出与“偏见挑战”及“缓解技术”相关的几个关键小节，供参考。

### 1. Introduction

1. **研究背景**
    - 简要介绍 AI 系统中存在的社会偏见问题，以及为何公平性在智能交互系统（或人工智能）中重要。
    - 引出视觉-语言模型（VLM）在多模态领域的快速发展，以及其潜在的偏见与风险。
2. **研究目的和意义**
    - 本文主要围绕视觉-语言模型的公平与偏见展开系统文献回顾。
    - 指明该主题对于学术界与工业界的重要性。

### 2. Research Questions

- 列出本文主要回答的研究问题（如上给出的 RQ1、RQ2、RQ3）。
- 简要说明这些问题的动机及文章将如何尝试回答。

### 3. Search Strategy

1. **检索数据库与平台**
    - 指定使用的数据库（如 ACM Digital Library 等），并说明选择原因。
2. **检索关键词**
    - 示例：("Vision-Language Models" OR "Multimodal") AND ("bias" OR "fairness" OR "social bias")。
3. **时间范围**
    - 默认回顾近 3 年（例如 2020 年至今），若文献不足可扩展至近 5 年。
4. **筛选标准**
    - 说明排除重复、与主题相关度低等文献的方式；最终保留多少篇文献。

### 4. Summary of Research

本节是整篇文献综述的核心，可按如下子部分组织：

#### 4.1 偏见与公平性概念在 VLM 场景下的特殊性

- 简述为什么视觉-语言模型相较于纯语言模型或纯视觉模型更易（或更难）产生偏见。
- 结合已有研究，讨论多模态数据的标注偏差、训练流程中语言先验与视觉先验的冲突等问题。

#### 4.2 偏见的主要类型和表现形式

- 汇总文献中报道的典型社会偏见（如性别刻板印象、种族标签错误、非人化标签等）以及在实际应用/数据集上的具体事例。
- 可能引用：CLIP、BLIP、Stable Diffusion 等模型中出现的偏见现象案例。

#### 4.3 偏见的测量与评估方法

- 内在测量：如 Grounded-WEAT、iEAT，以及多模态空间中的嵌入分析。
- 外在测量：零样本分类错误率差异、图文检索中的人口统计分布偏差、文本-图像生成中的性别/种族歧视等。
- 对比各测评指标的优缺点及适用范围。

#### 4.4 偏见缓解技术

- **数据级方法**：对图文对齐数据进行增强或清洗；对不平衡敏感属性进行重新采样。
- **模型级方法**：对抗训练、多任务正则、视觉/文本特征解耦等。
- **推断级方法**：向量投影去除敏感特征、文本提示后处理、额外微调模块减少偏见。
- 结合已有实证，讨论这些方法在多模态场景下的优势和不足。

#### 4.5 多属性多模态场景中的公平性挑战

- 扩展讨论同时涉及性别、种族、年龄、宗教等多种敏感属性时可能的复杂性。
- 提及现有对多属性联合去偏研究的缺失与潜在方向。

### 5. Conclusions

1. **主要发现**
    - 结合前文文献讨论，简要回答研究问题 RQ1、RQ2（及 RQ3）。
2. **研究局限与未来展望**
    - 现有研究在哪些方面仍显不足（数据规模、方法通用性、跨文化适用性等），以及后续可能的改进方向。
    - 强调进一步研究对真实世界部署和多元群体影响的重要性。

### 6. References

- 按指定的参考文献格式列出所有引用过的论文、报告等。
- 可以使用 BibTeX 或其他文献管理工具来自动生成参考文献列表。

