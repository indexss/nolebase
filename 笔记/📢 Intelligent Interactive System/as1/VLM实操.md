# VLM实操
## Intro
多模态模型近年来取得了显著进展，其中属VLM模型应用最为广泛，跨模态对齐、图像检索、文本生成、图像生成等任务中展现了强大的能力。然而，随着VLMs模型的应用，其潜在的偏见与公平性问题也引发了关注。研究表明，VLMs可能会继承甚至放大训练数据中的bias，导致社会不公正，影响用户信任和实际应用的可靠性。本survey聚焦于VLM的偏见和公平性问题，首先，我们探讨了VLMs在不同层次上偏见的体现以及来源，接下来，我分析了不同模型在不同任务和模态下展现的不同偏见模式，最后，我调查了VLMs的去偏方法。希望本综述可以为VLMs公平性的研究提供经验总结。

## 系统搜索
本综述对系统搜索了VLM领域的偏见和公平性文献。我在多个文献数据库中搜索了相关文献，包括ACM Digital Library，Google Scholar以及arXiv。在检索过程中，我使用了两组并列的关键字，第一组搜索词为：vision language model，vision-language model, VLM，multimodal。这些关键词保证了搜索结果集中在VLM这一领域。第二组关键词为：bias, fairness, discrimination, dataset bias, model bias, algorithmic discrimination。在筛选过程中，我主要关注最近3年内(2021-2024) 发表的文献，以保证研究的时效性。同时，我也排除了泛泛讨论人工智能公平性但未涉及VLM的文献。然而，为了提供完整的背景信息，本调查在介绍经典理论时，也适当引用了3年以上的文献，这一策略确保了本调查的广泛性与针对性，页避免了与其他ML领域的公平性研究相混淆。

## 研究问题
针对VLMs的偏见以及公平性，我们提出以下三个研究问题
0 VLMs及其下游任务中的偏见与不公如何体现？其来源是什么？
0 不同VLMs在不同任务不同模态下偏见形式与强度是否不同？
0 现有哪些方法能够减轻VLMs中的偏见？

1. VLM本身与其下游任务中存在的社会偏见如何体现？其原因与来源是什么？
	1. 先介绍本身以及下游任务。zhu2024generalized
		1. 本身任务这些任务通常用于 **训练 VLM（如 CLIP、BLIP、Flamingo、GPT-4V）**，属于 **基础任务**，主要目标是学习 **跨模态（Vision-Language）对齐**
		2. 下游任务VLM 预训练完成后，可以用于许多不同的下游任务，这些任务可能会引入新的偏见，VLM 作为特征提取器或基础模型
		3. zero-shot检索属于本身的偏见
		4. 文本生成 图片生成属于下游任务
	2. VLM在作为编码器层面就有偏见。
		1. zhang2024joint，CLIP普遍将职业与男性嵌入的距离比职业和女性远
		2. hamidieh2024identifying，恐怖分子-》中东男性，家庭主妇-〉印度女性
		3. jung2024unified，“穿西装的人”更可能检索到男性图片。
		4. seth2023dear，CLIP 在“医生”和“护士”文本描述时，更多地将“医生”与男性匹配，而将“护士”与女性匹配。
		5. ananthram2024see，一幅中国画在英文标注下被标记为“喜悦”，在中文标注下被标记为“悲伤”，但 VLM 
	3. VLM在多种下游任务中都存在社会偏见。
		1. 图像字幕生成，
			1. jung2024unified，运动员经常被描述为男性
			2. zhang2024think，模型会错误地将“果汁”与女性相关联，将“足球”与男性相关联。
		2. 文本图像生成，
			1. jung2024unified，程序员被生成为男性，即使用中性提示词
			2. sathe2024unified，DALL-E-3汇总职业角色，科学家多位男性，护士多位女性
		3. 视觉问答
			1. wolfe2022american，96.7白人被认为是美国人，2.8%亚裔为美国人
			2. ananthram2024see，VLMs 在 VQAv2（西方概念）数据上的表现优于 VLUE（包含中国文化概念的数据）。
		4. 视觉推理
			1. abdollahi2024gabinsight，    - **如果画面只有女性**，模型能够正确识别活动并生成正确的描述。**如果画面中同时有男性和女性**，即使女性正在修理设备，模型仍然倾向于认为**男性是执行者**。
			2. lee2024visionlanguage，让 VLM 根据不同肤色的黑人个体生成故事，并计算相似性。深色皮肤个体的故事更趋同质化。
	4. 来源是什么？
		1. 训练数据：继承数据中的bias，数据集有偏见。
			1. 标签bias：真实世界的大规模数据集通常具有长尾分布，即大多数标签只与少量样本相关联，零样本 CLIP 在 ImageNet-1K 数据集上的预测严重倾斜，某一类别（类别 0）被过度预测超过 3500 次，是该类别实际样本数量的三倍。zhu2024debiasing
			2. 虚假相关：地理偏差，子类偏差，风格偏差，敏感受保护属性zhu2024debiasing
			3. 社会bias：模型学习到的关联反映了社会刻板印象或偏见zhu2024debiasing
			4. 多模态数据
				1. VL 数据集往往包含带有偏见的标注（如带有刻板印象的图像标题），并且真实世界数据分布不均衡。lee2023surveya
				2. 图像与文本的表达形式不同，文本可描述更多抽象特征，而图像只提供可见信息。跨模态对照或“对抗样本”（如仅变换性别或种族信息）难度大。lee2023surveya
		2. 模型问题
			1. - **更大的 Transformer 模型（如 CLIP ViT-L14）在有害预测上更自信**，即使预测是错误的。
			2. gavrikov2024are与仅具有视觉功能的模型相比，VLMs通常表现出更强的形状偏见。这表明，在VLMs中整合文本信息可以影响并潜在地改变固有的视觉偏见。
			3. weng2024images研究结果显示，图像特征对偏见的贡献显著高于文本特征，在MSCOCO和PASCAL-SENTENCE数据集中分别占32.57%和12.63%的偏见。这突显了视觉和文本模态的结构集成如何不同地影响模型内的偏见。
			4. 不同的VLM会有不同的偏见形式，下文介绍。
2. 不同的VLM体现的偏见是否有不同的偏向？
	1. 每个模型偏见的方式不同，并不是在一个最偏见，另一个也偏见。
	2. hamidieh2024identifying提出 So-B-IT 偏见分类体系，将偏见分为了十种类型的偏见，并使用了C-ASC归一化指标计算了每个类别的偏见程度，并将四种CLIP变体进行了衡量。结果表明OpenCLIP 偏见最严重特别是在职业类别，而虽然DebiasCLIP 成功减少性别偏见，但加剧了种族偏见。
	3. sathe2024unified提出了一个偏见评估框架以及一个新的偏见度量指标Neutrality，对GPT-4V、GeminiProVision、LLaVA、ViPLLaVA、CoDi、SDXL、DALL-E-2、DALL-E-3等模型在多个任务上进行了衡量。结果显示，
		1. **不同模型在不同模态（Text-to-Image、Image-to-Text、Text-to-Text、Image-to-Image）下的偏见方向和强度不同**。**某些模型在某个偏见维度上较好，而在另一个偏见维度上较差**，即**偏见表现并不一致**。
		2. 在性别偏见方面，**LLaVA 和 CoDi 更倾向于男性，而 ViPLLaVA 更倾向于女性**，即不同模型在性别偏见上的方向不同。
		3. 在种族偏见方面，**DALL-E-3、SDXL 和大多数 VLMs 生成的职业图像主要是白人**，但 **CoDi 在 Image-to-Text 任务中偏向非洲裔**，说明 **不同模型的偏见方向可能不同**。
		4. 年龄偏见方面，**DALL-E-3、SDXL、LLaVA、ViPLLaVA 偏向生成年轻人（18-44 岁），但 CoDi 更倾向于生成中年人（45-64 岁）**。
		5. **开源模型（如 LLaVA、ViPLLaVA、CoDi）通常表现出更严重的偏见，而专有模型（如 GPT-4V、GeminiProVision）偏见较小**，但仍然存在一定的偏见模式。
		6. **跨模态任务（Text-to-Image、Image-to-Image）偏见比单模态任务（Text-to-Text、Image-to-Text）更严重**，因为这些任务需要模型从一种模态推测另一种模态的信息，容易产生刻板印象。
	4. raj2024biasdora**提出了一种系统性探测VLM偏见的新方法**，涵盖**T2T、T2I、I2T**三种模式，对比了GPT-4O, LLAMA-3-8B, DALL-E 3, Stable Diffusion, LLAVA等模型。结果显示，GPT-4O在**种族、性取向和宗教**上产生更多负面偏见，**但在性别和外貌上比 LLAMA 更少出现极端偏见**。而LLAVA在描述残疾人、外貌和性取向时的负面性比 GPT-4O 更明显。 
	5. wang2024vlbiasbench提出**VLBiasBench**，一个用于评测LVLMs偏见的综合性基准，涵盖**九大独立偏见类别**及**两个交叉偏见类别**，并使用开放式评测与封闭式评测的方法进行评估。结果表明，在开放式测评中，**InstructBlip-flan-t5-xl** 在**种族偏见（Race Bias）**上偏见最大（排名第一），但在**职业偏见**上表现较好。**Blip2-flan-t5-xl** 在**宗教偏见（Religion Bias）**上偏见最严重（排名第一），但在**种族偏见**上的排名相对靠后，说明它对不同类别的偏见程度不同。在封闭式测评中，**LLaVA-1.5-13B** 在**职业偏见（Profession Bias）**上的正确率较高，但在**社会经济地位偏见（SES Bias）**上偏见较大，- **Minigpt4-vicuna-7B** 在**职业偏见（Profession Bias）**的正确率最低，但在**宗教偏见（Religion Bias）**上表现较好。另外，在文本主导的数据集（Base）和图像主导的数据集（Scene）之间，不同模型的表现不同- **Gemini 在图像驱动的测试（Scene）上表现优异，但在文本驱动的测试（Base）上较差**，表明它更依赖视觉理解。**Shikra-7b 在文本驱动的测试（Base）上表现较好，但在图像测试（Scene）上的偏见较严重**。
3. 如何de-bias？
	1. pre
		1. zhang2022counterfactually 通过对抗攻击在数据阶段减少偏见，对于图像模态，通过对抗攻击改变图像中的性别特征而不改变其他信息，对于文本模态，直接**替换文本中的性别相关词汇**生成反事实文本使模型学习的概念不依赖于性别词汇，从而减少偏见。
		2. ananthram2024see  - 通过在 VLM 训练时引入更平衡的语言数据（如更多的中文文本）证明了**在预训练阶段就采用多语言文本能更有效减少偏见**，比简单的提示工程更具有普适性。
		3. howard2024socialcounterfactuals **通过 Stable Diffusion 生成对比样本**，创建高质量、仅改变社会属性的反事实SocialCounterfactuals数据集。规模大（17.1 万对）**利用该数据集对 VLM 进行训练，从而减少模型在交叉社会属性上的偏见**。
	2. in
		1. berg2022prompt 模型层面，提出了一种**可学习提示 + 对抗去偏 + ITC联合训练**方法，属于**模型层面（In-processing）**的方法
		2. seth2023dear DeAR 训练**对抗性残差网络**，在视觉编码器的图像表示上添加残差调整。仅需训练一个小型残差模块，无需重新训练整个 VLM，可与任何 VLM结合使用，尽可能保持零样本能力。训练复杂，超参数敏感。
		3. jung2024unified - - **仅需训练一个随机森林分类器**（计算敏感特征），训练时间**仅几十秒**。**无需重新训练**，可以直接在**冻结的VLM表征**上应用，节省计算资源。保持模型语义完整性，避免误删重要信息
		4. weng2024images MTCNN（面部检测）** + **MobileNet（性别分类）** 识别并替换人脸区域，模糊性别特征。通过替换性别词并计算其反向性别词特征均值，减少文本的性别信息影响。**计算代价小**，下有效减少偏见
	3. post
		1. chuang2023debiasing 提出了一种基于文本嵌入投影的方法，通过提示词测量偏见并通过正交投影去偏。其证明了校准的投影矩阵可以等价于一个均衡化损失，确保模型不会偏向特定群体。
		2. janghorbani2023multimodal  - 通过计算**模型嵌入的不同维度对偏见的贡献**，选取对偏见贡献最大的 N 个维度，并评估它们对分类的影响，最后移除影响偏见的冗余维度，减少刻板印象偏差。仅需处理嵌入空间，不影响推理速度，偏见减少高达**93%**，准确率仅下降1.1%（MMBias 数据集）和 1.3%（CIFAR-100 数据集）