# Benchmarking and Evaluation
模型在不同的环节下需要测量的指标是不一样的

![](assets/Pasted%20image%2020241202031531.webp)


## Close Ended - Text Classification
Close Ended的特点就是：
1. **答案数量有限**：
    - 任务的潜在答案数量是有限的，可能是多个选项中的一个或少数几个。
    - 例如：选择题、判断题或分类任务。
2. **通常只有一个或少数正确答案**：
    - 大多数情况下，封闭式任务只有一个明确的正确答案，或者仅有少量的正确答案。
    - 这种特性使得任务的评估具有高确定性。
3. **支持自动化评估**：
    - 由于答案是有限且明确的，可以使用机器学习（ML）或其他自动化方法进行评估。
    - 例如，模型的预测结果可以直接与正确答案进行对比，从而计算准确率等指标。

一些Close Ended问题分类，比如有
- Sentiment Analysis：情绪判断，正面还是负面
- Entailment：第一句是否能推出第二局的结论
- Name entity recognition: CoNLL-2003：从文本中识别出特定类别的实体，例如人名、地名、组织名等。
- Part-of-Speech: PTB : 词性判断
- Coreference resolution: WSC: 判断代词指的是哪一个
- Ques;on Answering: Squad 2: 问答，封闭式问答任务的答案通常是明确的，例如一个短语或单词，而不是开放式的自由文本生成。

而在close ended领域，最棒的benchmark就是这个多任务benchmark：superGLUE, 涵盖了多领域的任务。
![](assets/Pasted%20image%2020241202032226.webp)
![](assets/Pasted%20image%2020241202032234.webp)
而Close ended评估也有一些挑战：
-  选择指标：准确度？回归率？precision？f1-score？ROC
-  跨指标或任务的聚合：在多任务评估中，需要将不同任务的结果进行聚合以得出综合性能。不同任务可能使用不同的指标，如何合理地加权和整合是一个难题。SuperGLUE任务中，使用多种指标（如F1、准确率、Matthew's Correlation）评估不同子任务的性能。通过加权平均或其他方法，将这些指标整合为一个综合分数。
-  标签来源
-  潜在的虚假相关性 spurious correlations：模型可能会利用数据中的虚假相关性来获得高分，而不是通过真正理解任务。例如，在情感分析任务中，某些特定词（如“amazing”）可能总是与正面情感相关，模型可能只依赖这些词而忽略上下文。

## Open Ended - Text Generation
Open Ended的任务可能有多种正确答案，答案的质量也有高低之分，所以不能用传统的ML指标。
例如
- 文本摘要领域
	- **CNN-DM**：一个新闻摘要数据集，包含CNN和Daily Mail的新闻文章及其摘要。
	- **Gigaword**：一个更大规模的新闻摘要数据集，主要用于训练和评估摘要生成模型。
- 机器翻译领域
	- **WMT**（Workshop on Machine Translation）：一个广泛使用的机器翻译数据集，包含多种语言对的平行语料。
- 指令理解与执行（Instruction-following):
	- **Chatbot Arena**：用于评估对话生成模型的响应质量。
    - **AlpacaEval**：一个指令跟随任务的评估框架。
    - **MT-Bench**：用于评估多任务模型在完成不同任务时的性能。

在文字生成领域，我们一般有三种评判标准：
### 内容重合度指标
就是用不同的方式算生成与标签之间的重合度。速度很快，可以用N-gram指标 [1 - Word Vectors](1%20-%20Word%20Vectors.md)
- **常用的 n-gram 重叠指标**：
    - **BLEU**（Bilingual Evaluation Understudy）：主要用于机器翻译，基于 n-gram 精确率（precision）（在所有被模型预测为正类的样本中，实际为正类的比例。）。
    - **ROUGE**（Recall-Oriented Understudy for Gisting Evaluation）：主要用于文本摘要，基于 n-gram 召回率（recall）（在所有实际为正类的样本中，被模型正确预测为正类的比例。）。
    - **METEOR**：结合精确率和召回率，同时考虑词形变化（如复数形式）。
    - **CIDEr**：专为图像描述生成任务设计，强调语义相关性。

![](assets/Pasted%20image%2020241202033005.webp)

### Model-based 指标
基于模型的评估指标的核心思想是：
- **Learned Representations**：
    - 使用预训练模型生成单词或句子的表示（Embeddings），这些表示捕捉了文本的语义信息，而不仅仅是词汇层面的匹配。
    - 通过这些表示，可以计算生成文本与参考文本之间的语义相似度。
- **语义相似性计算**：
    - 通过比较生成文本和参考文本的嵌入表示（例如句向量），可以量化它们在语义上的接近程度。
    - 这种方法更关注语义，而不是仅仅依赖于词汇或短语的重叠。

#### Reference-based Evaluation
有哪些指标呢？
##### Vector Similarity
没什么好说的。
![](assets/Pasted%20image%2020241202033522.webp)

##### BERTSCORE
![](assets/Pasted%20image%2020241202033614.webp)
使用预训练BERT去按照右面的公式计算分数。

##### BLEURT
![](assets/Pasted%20image%2020241202033722.webp)
BLEURT 是一种自然语言生成的评估指标。它以一对句子作为输入，一个参考句和一个候选句，并返回一个分数，指示候选句在多大程度上流畅并传达了参考句的意思。
- **测试偏差增加会降低所有指标的性能**，因为任务变得更困难。
- **训练偏差对未预训练的 BLEURT 有灾难性影响**，但经过预训练的 BLEURT 更加鲁棒。
- **预训练的重要性**：预训练使 BLEURT 能够学习更广泛的语义信息，从而在训练和测试分布不一致的情况下表现更好。

然而，Reference-based方法有一定的问题，比如说**ROUGE**（Recall-Oriented Understudy for Gisting Evaluation）：主要用于文本摘要指标。
![](assets/Pasted%20image%2020241202034947.webp)
专家生成的参考摘要质量更高，内容更忠实于原文，Rouge-L 分数较高，同时这些生成摘要的忠实性也更高，与人类评分一致。
所以，
- Rouge-L 的有效性取决于参考摘要的质量。
- 为了提高基于参考评估的可靠性，应优先构建或选择高质量、忠实的参考摘要。
- 这项研究强调了数据质量对自动化评估指标的重要影响。

所以我们说，人类参与评估的方法，优缺点是什么呢？
- 人类评估仍然是最终标准
- 难以标准化，每个人的评估标准不一样，无法跨paper研究，因为你不知道别人的评分标准是什么。有研究指出，就算是有原文作者的帮助，另一篇论文和原文的人类评分结果能差最少80%。不可重复
所以我们想要不用人。
#### Reference free evaluation
无需参考的方法，不需要人类撰稿，而是使用一个模型去给出分数，曾经不标准 - 现在在GPT4中变得流行。例子是AlpacaEval, MT-Bench

##### Chatbot Arena - side-by-side human eval
让人们并排玩两个模型，给出点赞或点踩的评分。这算是LLM评估的金标准。
但是在网站上输入随机问题可能不具代表性。
而且，特别特别慢，难以很快就给出一个分数，且只对著名模型进行benchmark。
![](assets/Pasted%20image%2020241202035811.webp)

所以，我们就像采用LM作为evaluator，去评价别人的模型。Surprisingly效果很好，常见的版本有AlpacaEval, MT-bench

##### AlpacaFarm
![](assets/Pasted%20image%2020241202040347.webp)
全自动化的 AlpacaEval 仅需花费约 **1/22 的经济成本**和 **1/25 的时间成本**，就可以达到很高的人类赞同度。
另一方面，这个paper也说明了人类是高Bias高Variance的

![](assets/Pasted%20image%2020241202041146.webp)

而下面的图也说明了，人类注释器和模拟注释器都更偏好 **列表形式** 和 **较长的输出**，这可能是因为这两种格式更易于阅读和理解。
![](assets/Pasted%20image%2020241202041417.webp)
这也揭露了更多问题，比如说 Spurious correlations
**(a) Length（长度）**
- **问题**：较长的输出可能被人类和模拟注释器高估，因为它们看起来更详细、更全面。
- **解释**：
    - 人类和模拟注释器偏好较长输出（右图中偏好百分比接近 75%），但这可能并不总是因为较长的输出更好，而是因为长度在视觉上给人一种“更完整”的错觉。
    - 这种偏好可能掩盖了短而精确的输出的优点，导致模型在生成时倾向于冗长的回答。
**(b) Position（位置）**
- **问题**：输出内容的顺序或位置可能影响偏好，但这一问题通常通过随机化（randomization）来消除。
- **解释**：
    - 如果列表或输出的顺序没有随机化，注释器可能会对较早出现的选项产生偏好（例如，首因效应）。
    - 研究中通常会通过随机排列选项来避免这一问题，因此这里提到“everyone randomizes this away”（每个人都通过随机化消除了这一问题）。
**(c) GPT-4 Self Bias（GPT-4 的自偏性）**
- **问题**：GPT-4 可能对自身生成的输出存在偏好。
- **解释**：
    - 如果 GPT-4 被用作注释器或参考模型，它可能倾向于更高评价自己生成的内容。
    - 这种偏好会导致偏倚，使得 GPT-4 的输出在评价中显得更优，而实际上可能并非如此。
    - 这种现象可能在灰色点（其他模拟注释器）中体现出来，部分注释器对较长输出或列表形式的偏好可能与 GPT-4 的自偏性相关。

下面就有了AlpacaEval
 **AlpacaEval 的特点**
- **内部基准**：用于 Alpaca 模型的开发和改进。
- **高相关性**：与 Chatbot Arena 的评估结果相关性高达 98%，表明其评估结果具有高度可靠性。
- **快速且低成本**：
    - 单次评估耗时 **小于 3 分钟**。
    - 每次评估成本 **小于 $10**。

其流程为：
1. 生成输出。
2. 通过 GPT-4 评估优劣。
3. 长度校正（可选）。根据输出长度重新加权胜率
4. 计算平均胜率。
AlpacaEval通过Length-controlled来降低长度带来的虚假相关。
![](assets/Pasted%20image%2020241202041930.webp)

另一个需要担心的就是，模型是否会对自己生成的内容更加偏好？也就是self-bias? 还好，不存在这样的问题。

![](assets/Pasted%20image%2020241202042015.webp)
## LLM Evaluations Now
### HELM and open-llm leaderboard
收集许多可以自动评估的基准，生成新的评估。
![](assets/Pasted%20image%2020241202042120.webp)
有哪些LM数据集呢？
- **NarrativeQA**：短答案问答，基于书籍和电影脚本。
- **NaturalQuestions（closed-book 和 open-book）**：短答案问答，基于维基百科或搜索查询。
- **OpenbookQA**：选择题问答，关注小学科学知识。
- **MMLU**：多任务语言理解，涵盖多个领域（如数学、科学、历史等）。
- **GSM8K 和 MATH**：数学推理与计算。
- **LegalBench 和 MedQA**：法律和医学领域的选择题问答。
- **WMT 2014**：机器翻译，基于多语种句子。

### Massive Multitask Language Understanding (MMLU)
用于衡量语言模型在57个多样化知识密集型任务上表现的新基准

### Code Capabilities
- **特点**：
    - 语言模型可以生成代码，且代码的质量可以通过测试用例直接评估。
    - **评估指标**：
        - **Pass@1**：表示生成的代码输出中，至少有 1 个符合测试用例要求的比例。
        - GPT-4 的 Pass@1 大约为 67%。
- **评估工具**：
    - **HumanEval**：一个包含人类编写的代码生成评估基准，提供测试用例来验证代码的正确性。

### Agent Capabilities
- **描述**：
    - 智能体是指能够执行超越文本生成的任务的模型，例如在虚拟环境中完成操作。
    - **AgentBoard** 显示了多种任务和评估指标：
        - **任务类型**：如 Web 搜索、工具操作、虚拟环境交互等。
        - **评估指标**：成功率、进度率、记忆能力、规划能力等。
- **挑战**：
    - 智能体的评估需要在沙盒环境中进行，确保操作的可控性和安全性。
![](assets/Pasted%20image%2020241202042738.webp)

### Perplexity
困惑度与下游性能高度相关（r2=0.940），但依赖于数据和分词器的选择。
![](assets/Pasted%20image%2020241202042802.webp)
### Arena-like
- **描述**：
    - Arena-like 排行榜是用户驱动的评估方式，用户通过比较模型的输出来投票决定模型的优劣。
    - **展示内容**：
        - 排行榜显示了多个模型的 Elo 分数、投票数量、组织来源、知识截止日期等信息。
        - 用户根据模型的表现进行打分，最终决定模型排名。
![](assets/Pasted%20image%2020241202042906.webp)
## Challenges with Evaluation
### Consistency issues
**一致性问题**是指在评估语言模型性能时，由于测试设置或实现方式的差异，导致同一模型在相同任务或基准测试中表现和排名出现显著变化的问题。
模型的表现对问题设置和测试实现方式非常敏感。比如：
- **示例问题**：沙特阿拉伯的首都是什么？
- **两种设置**：
    1. **稀有符号选项**：选项中包含不常见的符号（如 "ø" 和 "ü"）。
    2. **固定答案选项（B）**：标准化选项，正确答案固定为 B。
- **模型排名变化**：
    - 在稀有符号选项中，模型的表现和排名与固定答案选项中存在显著差异。
    - 例如，某些模型在稀有符号选项中表现较差，但在固定答案选项中表现更好。

#### MMLU一致性问题
这个问题在MMLU上就有体现。MMLU 的提示词和评估方式的差异会显著影响模型的得分和排名，导致结果不具有可比性。
- MMLU 数据集在不同实现（如原始版本、HELM 和 EleutherAI LM Harness）中的提示词结构存在差异。
- 尽管这些差异看似细微，但它们会显著影响模型的表现。
比如：**提示词差异**：
    - **原始实现**：直接提供问题和答案选项。
    - **HELM**：增加了 "Question:" 前缀，并在问题前添加了额外的空格。
    - **EleutherAI LM Harness**：在答案选项前添加了 "Choices" 关键词。
这些细微的修改可能会改变模型对问题的理解方式，进而影响其预测结果。

1. **原始实现**：
    - 比较模型对四个选项（A、B、C、D）的概率分布，选择概率最高的选项。
    - **优点**：限制模型的答案范围，有助于提高准确性。
    - **问题**：模型可能更倾向于生成不在选项中的词汇（如 "Zygote"），但由于限制，它会选择一个选项，从而得分。
2. **HELM 实现**：
    - 使用模型生成的下一个 token 的概率来选择答案。
    - 如果生成的答案不在选项中（如 "Zygote"），则直接判定为错误。
    - **问题**：这种方法更严格，但可能对模型生成的答案不够宽容。
3. **EleutherAI LM Harness**：
    - 比较完整答案序列的概率（如 "C. The second pharyngeal arch"），而不是单独的选项。
    - 使用对数概率的加和来计算完整序列的概率，并可能对序列长度进行归一化处理。
    - **问题**：这种方法可能对较长的答案不公平，但归一化可以缓解这一问题。
![](assets/Pasted%20image%2020241202043734.webp)
### Contamination and overfitting issues
数据污染指的是模型在训练阶段接触到了测试数据，导致其在评估时表现异常好，但这并不能反映模型的真实泛化能力。
Phi-1.5 模型可能过拟合于特定基准测试（如 GSM8K，一个数学问题数据集）。她举例说明，如果将一个问题输入 Phi-1.5，模型能够正确自动补全答案。
![](assets/Pasted%20image%2020241202044102.webp)
所以，我们很难保证，一个benchmark的答案有没有被大模型提前知道，从而overfitting
然而，也有了一些解决方法
- Private test set。这个保证新
- Dynamic test set：测试集的数据输入是动态生成的，开发者无法预先知道测试数据的具体内容。可以采用类似dynabench的框架：
    1. 编写新的测试样例。
    2. 使用模型反馈调整问题。
    3. 验证问题是否有效。
    4. 用新问题评估模型并进行下一轮训练。
也有一些检测是否被污染的方法：
- Min-k-prob
	- **核心思想**：检测模型是否在训练中接触过特定测试数据，通过检查模型对文本的生成概率是否异常高来判断。
	- **具体步骤**：
	    1. **获取 Token 概率**：
	        - 给定一段文本 XX（如图中的“Miss Universe Thailand”句子），让模型（如 GPT-3.5）计算每个 token 的概率分布。
	    2. **选择最低概率的 k% Tokens**：
	        - 从所有 token 的概率中，选择最低的 k%。
	    3. **计算平均对数似然**：
	        - 对这些最低概率的 token 计算平均对数似然值。
	    4. **判断污染**：
	        - 如果平均对数似然值异常高，则可能表明模型在训练中见过该文本或其变体，从而怀疑数据污染。
![](assets/Pasted%20image%2020241202044553.webp)

- Exchangeability test
  通过改变测试数据的顺序，观察模型在不同排列下的表现是否一致，来判断模型是否对特定数据有记忆。
![](assets/Pasted%20image%2020241202044628.webp)

### Monoculture 问题
大多数LLM只在英语下评测了。我们应该采用多语言benchmark
![](assets/Pasted%20image%2020241202044730.webp)

### 局限于单一指标
只关注单一性能指标会导致对模型能力的片面理解，忽略其他重要维度。在评估模型时，应综合考虑性能、效率、公平性等多方面因素，以便更全面地反映模型的实际能力。评估方法应避免对少数群体的不公平，并能适应不同用户的需求和偏好。
MLPerf就是一个考虑了计算效率的metric
![](assets/Pasted%20image%2020241202044902.webp)

DiscrimEval是一个考虑biases的metric。
- **偏见来源**：模型在决策中可能因显式或隐式群体特征（如种族、性别）而表现出不同程度的偏见。
- **显式属性的影响更大**：直接提供群体信息时，模型更容易表现出偏见。
- **重要性**：通过模板化方法，DiscrimEval框架能够系统地评估模型对不同群体的偏见，为改进模型公平性提供了依据。
![](assets/Pasted%20image%2020241202045020.webp)

后面都是一些有关政治正确的内容，不想写了。。就这样吧


