以下以“来自数据集的偏差（Data-Level Biases）”与“来自模型/算法的偏差（Model-Level Biases）”为主线，对这篇综述所讨论的CV领域偏差做一个简要的双分法总结，并在每个大类里融合他们提到的社会层面、数据层面、以及人口学与非人口学偏差的要点。

---

## 一、来自数据集的偏差（Data-Level Biases）

1. **社会因素导致的数据不均衡**
    
    - 现实世界中对特定人群的歧视或不平等（如种族、性别、经济水平等），会通过互联网数据与公共数据集被带入模型训练。
    - 论文指出，许多大型公开数据集（如ImageNet、OpenImages、LAION等）往往偏重特定地域与主流群体，缺乏对全球或边缘群体的充分覆盖。
    - 社会层面的不平等，会在数据收集阶段被“放大”——例如昂贵物品更多地出现在富裕国家的照片中，使模型在推断时产生刻板印象。
2. **数据标注和采集策略本身的缺陷**
    
    - 文中提及，许多数据集缺乏对敏感属性（种族、性别、年龄等）的明确标注，或只能用“代理标签”，甚至标注者本身带有偏见；
    - 地理分布或采样渠道单一（如只用欧美社交媒体），导致某些群体/场景的图片明显不足；
    - 大规模自动爬取的数据可能携带仇恨/歧视性文本或不当信息（如LAION数据集中出现的hate speech），在训练中被学习后会再度传播。
3. **“人口学偏差”与“非人口学偏差”均能在数据层面出现**
    
    - **人口学偏差**（Demographic Bias）：如人脸识别训练集中对肤色较深人群数据不足、女性样本量少等；
    - **非人口学偏差**（Non-Demographic Bias）：如图像分类中背景/场景与目标标签强相关（厨房=“烹饪”、某种打光场景=特定类别）等，这些都是数据分布问题造成的虚假关联。

> **对应论文内容**：在“2.1 Bias Origins”、“3.1 Biases in Datasets”等章节，大量论述了数据偏差如何被采集、注释和放大；“5.1 Diversity of Datasets and Attributes”还进一步列举了用于公平性研究的典型数据集，以及不同属性在各种任务中的不平衡现象。

---

## 二、来自模型/算法的偏差（Model-Level Biases）

1. **机器学习过程中的“固有相关”与“伪相关”**
    
    - 论文利用图 2（Dependence Graphs）说明：
        - **Intrinsic dependence**：目标属性YYY与敏感属性SSS本身就相关，模型在追求高准确率的同时难以避免使用SSS，从而出现公平性-性能间的权衡；
        - **Spurious correlation**：模型错误地把某些敏感属性当作有效特征，或把背景场景当作主要判断依据（如在动作识别中只看背景），这会导致模型对非目标因素产生依赖。
    - 这种偏差不一定只局限于人口学特征，也可能是场景、背景、纹理等非人口学线索。
2. **模型设计与训练策略的放大或弱化**
    
    - **目标函数/损失函数**：若只追求总体准确率而忽视少数群体表现，模型通常倾向于在主流群体上取得最佳性能，而让边缘群体误差更大；
    - **训练策略/网络结构**：对抗式学习、数据再采样、正则化等不同方法，会影响模型最终对少数群体或敏感特征的表现；
    - 在“3.2 Biases in Models”章节，作者提到各种实验揭示预训练模型、压缩模型（如pruning）、自监督模型（SSL）等均会因训练过程细节产生不同程度的偏差。
3. **人口学与非人口学偏差在模型层面的表现**
    
    - 对人口学属性（如种族、性别）而言，模型若偏好某些特征，会导致对某些人群预测效果更差或出现歧视性错误；
    - 对非人口学属性（如背景、光照、服装），模型常常学到表面相关性，从而在测试时无法应对真实多样性，或对异于训练集分布的场景失效。

> **对应论文内容**：

- 在“2.1 Bias Origins”中，论文专门讨论了数据XXX、目标YYY、以及敏感属性SSS三者之间的依赖图；
- “3.2 Biases in Models”介绍商业人脸识别API、人脸表情识别、对象检测、预训练模型等在实测中存在的各种偏差现象；
- 另外作者也强调，在模型层面常见的一些缓解方法（如Adversarial Representation Learning、正则化、对抗扰动等）尝试切断SSS与YYY的关联。

---

## 小结

按照“来自数据集的偏差”与“来自模型的偏差”来阅读这篇Survey，可以更方便地理解作者对于CV偏差根源的划分与讨论：

1. **数据侧**：社会不平等 → 数据采集缺陷 → 不平衡或带歧视性的样本/标签 → 传递至模型；
2. **模型侧**：学习过程中的内在/伪相关，以及训练策略与网络结构等可能进一步放大或抑制偏差。

同时，在人口学与非人口学层面，论文也多次强调“对某些群体的歧视”与“对某些无关特征的依赖”都是CV中常见的问题。无论是人脸识别还是图像生成、VQA等任务，都体现了这两类偏差源头。该综述最后还介绍了去偏方法、去偏评测指标和最新研究趋势，帮助读者了解如何在实践中尽量减少这些偏差对模型决策的影响。