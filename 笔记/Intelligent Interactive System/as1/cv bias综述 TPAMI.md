https://arxiv.org/abs/2408.02464
---

## 1. 绪论（Introduction）

1. **计算机视觉的演进：**
    - 机器学习、统计方法为视觉识别带来突破
    - 大规模数据集与深度学习模型（CNN、Transformer等）推动了图像分类、分割、生成等各类任务的性能提升
2. **公平性与偏见问题：**
    
    - 随着计算机视觉在高风险领域的部署，需要确保模型不会延续或放大数据中的歧视倾向
    - 敏感属性（性别、种族、年龄等）与图像目标往往存在相关性，可能导致不公平预测
3. **论文目标：**
    
    - 梳理并总结计算机视觉中有关公平性和偏见缓解的主要工作
    - 系统阐述：
        1. 公平性的起源与定义
        2. 发现和分析视觉系统中的偏见
        3. 偏见缓解方法
        4. 与偏见分析相关的数据资源
        5. 多模态/生成式基础模型中的新趋势与挑战

---

## 2. 公平性的起源与定义（Origins and Definitions of Unfairness）

### 2.1 偏见的来源（Bias Origins）

1. **社会层面：**
    
    - 世界本身的不平等或歧视（例如网络数据多来自特定地区、社会群体）
    - 人工标注或互联网数据中暗含的刻板印象
2. **技术层面：**
    
    - 数据中固有的或意外产生的偏差：
        - **内在相关（Intrinsic Dependence）：** 目标属性 YY 与敏感属性 SS 天生相关
        - **虚假相关（Spurious Correlation）：** 目标与敏感属性在现实中本应独立却在数据中出现强相关
    - 模型训练过程可能放大数据偏差（训练目标、采样策略、网络结构等都会影响）
3. **区分：**
    
    - **人口学（社会）偏见：** 涉及性别、种族、年龄、肤色等
    - **非人口学（上下文/背景）偏见：** 场景、材质、背景等导致的偏差

### 2.2 公平性定义（Fairness Definitions）

1. **个体公平（Individual Fairness）：**
    
    - 对相似个体做出相似决策
    - 通过Lipschitz条件等约束度量
2. **群体公平（Group Fairness）：**
    
    - 使模型输出与敏感属性间相互独立或有条件独立
    - 包括：
        - **Demographic Parity（DP）：** 预测结果在不同群体分布一致
        - **Equal Opportunity（EO）：** 正样本预测一致
        - **Equality of Odds（EOO）：** 兼顾正负样本预测一致
3. **反事实公平（Counterfactual Fairness）：**
    
    - 对同一实例，如果仅更改敏感属性，预测结果应保持不变
    - 借由因果图和干预来形式化
4. **偏见放大（Bias Amplification）：**
    
    - 模型预测偏差超过训练数据本身的偏差，称为“放大”
    - 与数据中基准分布对比，衡量模型是否在训练中额外放大了差异

---

## 3. 偏见的发现与分析（Bias Discovery and Analysis）

### 3.1 数据集中的偏见（Biases in Datasets）

- 分析常用视觉数据集（ImageNet、Open Images、MSCOCO、CelebA等）中可能存在的性别、种族、地域、标注不均衡等问题
- 例：Buolamwini & Gebru [16] 指出面部数据集对深肤色群体不利；Shankar 等 [59] 指出ImageNet、Open Images存在地域偏差

### 3.2 模型中的偏见（Biases in Models）

- 商业化人脸识别系统对黑皮肤或女性识别率明显偏低
- 预训练视觉模型（CNN、ViT、CLIP等）在下游任务中继承甚至放大数据集偏差
- 一些工作提供审计框架来分析预训练模型的偏见来源以及对不同属性的敏感度

### 3.3 超越人口学属性的偏见（Biases Beyond Demographics）

- 数据集偏差：背景、纹理、上下文导致的“捷径”
- 模型倾向使用场景/物体等附带信息而非核心目标来做决策
- 针对VQA、动作识别等任务也常出现利用语言或场景线索的偏见

---

## 4. 偏见缓解方法（Bias Mitigation Methods）

### 4.1 通过不知情实现公平（Fairness through Unawareness）

- 直接去除或隐藏敏感属性（如模糊面孔）
- **局限**：其他可见线索依然可能与敏感属性高度相关；对人类相关任务影响尤其显著

### 4.2 公平表示学习（Fair Representation Learning）

1. **对抗式表示学习（Adversarial Learning）：**
    
    - 用对抗网络或对抗损失，最小化表示对敏感属性的可预测性
    - HSIC等非参数独立性度量可捕捉高阶统计依赖
2. **方法差异：**
    
    - 不同依赖度量（线性/非线性、均值/全分布）
    - 不同训练优化策略（零和对抗、闭式解、谱方法等）
    - 在人脸识别、图像分类、表示学习等广泛应用

### 4.3 准确率-不公平度的权衡（Accuracy-Unfairness Trade-Offs）

- 当目标标签 YY 与敏感属性 SS 关系密切时，会存在性能与公平之间的固有冲突
- 理论与实证均揭示：随着去除敏感属性信息的力度增强，模型对主任务的性能可能下降
- 有研究提出闭式方法或谱解来在不同公平度水平上求得最优表示

### 4.4 对抗式/反事实数据再平衡（Counterfactual Data Rebalancing）

- 利用GAN或其它生成式方法在数据层面进行“再平衡”
    - **生成/合成** 弱势组数据，以缓解数据不平衡
    - **交换背景/风格** 或利用因果方法消除偏见来源
- 实用场景：人脸识别、图像分类、场景理解、文本-图像任务等

### 4.5 分数校准与损失正则化（Score Calibration and Loss Regularization）

- 对决策分数后处理，使不同群体在阈值上更为公平
- 加入特殊损失或正则项（如最大均值差异、HSIC等），减少对敏感属性的依赖
- 在人脸识别（自适应Margin策略）、图像检索（后处理生成公平子集）等多领域应用

---

## 5. 数据集（Datasets）

### 5.1 数据集及属性多样性

- 大量通用数据集重复用于偏见研究：ImageNet、MSCOCO、CelebA、OpenImages等
- 主要关注的人口学敏感属性：**性别、种族/肤色、年龄**；其次是地域、背景、上下文等
- 女性感知、肤色、年龄相关的标注最常见，其他属性（如光照、纹理、发色）相对次要但同样重要

### 5.2 特定任务的数据集多样性

- **人脸识别/分析：** PPB、IJB-A、FairFace、UTKFace、RFW等（性别、肤色、年龄、种族）
- **图像分类：** Waterbird、Colored MNIST、CelebA等（背景、颜色、性别等）
- **动作识别：** UCF-101、HMDB-51、Diving48 等（场景/背景偏差）
- **多模态任务：** VQA、MSCOCO Caption、LAION 等（语言上下文、文本描述隐藏的偏见）
- **生成式模型：** CelebA、FairFace、StyleGAN衍生数据等（面部、性别、种族多样性）

---

## 6. 当前趋势与未来工作（Current Trends and Future Work）

1. **生成式模型的公平性：**
    
    - 大规模多模态数据与扩散模型/大模型兴起，带来更强生成能力
    - 目前主要聚焦于评估/提升生成结果在不同群体和文本描述下的多样性
    - **尚缺少** 对生成式模型正式的“公平性定义”和精确度量方法
2. **基础模型（Foundation Models）的偏见：**
    
    - 多模态大型模型（如CLIP、GPT-4V、LLaVA等）在零样本/少样本场景会携带显著偏见
    - 因适用任务广泛，需要统一、多层次的衡量标准与有效的解偏算法
    - **挑战：**
        - 广义开放集预测；
        - 词汇/属性开放；
        - 存在多维任务目标与不同敏感属性的耦合
3. **其他值得探索的方向：**
    
    - 采样策略与数据标注流程如何更系统地避免偏见
    - 因果推断方法与公平性之间的融合
    - 联邦学习、隐私保护与公平性的交叉研究

---

## 7. 总结（Concluding Remarks）

- 计算机视觉各个环节都可能产生或放大偏见：数据采集/标注、模型设计、训练范式等
- 近期研究在统计独立性度量、生成式/对抗式去偏、因果推断等方向快速发展
- 多模态/大模型时代的公平性仍有大量亟待解决的挑战
- 希望本综述能为研究者、从业者提供参考，推动社区在开发与部署公平可靠的视觉系统上更进一步



