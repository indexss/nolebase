# 1. 绪论（Introduction）

1.1 **背景与动机**

- 机器学习模型（尤其是基于 Transformer 的预训练模型）在自然语言处理（NLP）和计算机视觉（CV）等领域取得了巨大进展。
- 研究者发现，模型会从训练数据中学习并强化其中的社会偏见（social biases），可能对社会群体造成资源分配不均或不公正的代表性（representational harm）。
- 保障 AI 系统的公平性、减少这些偏见成为学术界与工业界的重要议题。

1.2 **多模态领域的兴起与挑战**

- 随着视觉-语言（Vision-and-Language, VL）预训练模型的发展，如何在多模态模型中识别和缓解社会偏见的需求也变得紧迫。
- 目前针对单模态（NLP 和 CV）的研究相对成熟，而多模态（VL）的偏见问题尚未有相同深度的研究与讨论。
- 本综述旨在归纳 NLP、CV、VL 三方面的社会偏见研究进展，提供高层次见解并给出潜在的指引。

---

# 2. 社会偏见研究的总体概览（Overview）

2.1 **概念与术语（Concept and Terminology）**

- 模型中需要的“归纳偏置”（inductive bias）本质上与“社会偏见”不同；社会偏见会导致对某些群体的歧视或不公。
- 受保护属性（protected attributes）是法律或道德层面上不可被模型歧视的敏感因素，如性别、种族等。

2.2 **常见受保护属性：性别/种族（Protected Demographic Attributes）**

- 2.2.1 **性别/生理性别子群体**
    - 常见研究多采用二元（男性/女性）分类，但也有工作开始纳入跨性别、非二元人群。
    - 真实世界中性别是社会建构，与生理性别有所区别，且不同文化可能有更多元的性别定义。
- 2.2.2 **种族/民族子群体**
    - NLP 中多从“白人/黑人”或包含更多族裔（如亚洲、印第安裔、拉美裔等）定义出发。
    - CV 中因种族/民族定义争议较多，常用肤色量化方式（Fitzpatrick、Monk Skin Tone）来代表不同深浅的肤色，进而衡量模型在不同肤色群体上的公平性。

2.3 **公平准则（Fairness Criterion）**

- **人口统计平等**（Demographic Parity）、**机会均等**（Equality of Opportunity）、**等价机会**（Equality of Odds）是常用的数学形式化准则，用于确保模型对不同群体的决策结果无显著差异。

2.4 **偏见衡量与缓解方法总览（Categorization of Bias Metrics and Mitigation Methods）**

- 2.4.1 **衡量指标（Bias Metrics）**
    - **Intrinsic（内在）**：检测模型本身（如词向量或图像特征）是否存在偏见（如 WEAT、iEAT 等）。性能可能下降，争议比较大
    - **Extrinsic（外在）**：考察模型在下游任务（如分类、生成、检索等）中的性能差异或歧视行为。
- 2.4.2 **偏见缓解（Mitigation）方法**
    - **数据级（Pre-processing）**：通过数据增强、去敏感化或平衡训练数据等手段处理数据本身。
    - **模型级（In-processing）**：在模型训练过程中修改优化目标（如对抗训练、约束优化、正则化等）。
    - **推断级（Post-processing）**：在模型训练完成后，通过后处理、矢量变换、重分配标签等方式减少偏见。

---

# 3. 单模态模型中的偏见（Bias in Unimodal Models）

3.1 **语言模型中的偏见（Bias in Language）**

- 3.1.1 **衡量与分析**
    - **内在指标**：
        - WEAT/SEAT/CEAT 等，基于词向量或句向量检测男女、种族等词之间的相似度差异。
        - StereoSet、CrowS-Pairs 等数据集，检测语言模型在刻板印象语句与反刻板印象语句之间的倾向性。
    - **外在指标**：
        - 下游任务（如分类、对话、机器翻译等）中比较不同群体的错误率或模型输出质量差异（例如 Winobias、Winogender）。
- 3.1.2 **缓解方法**
    - **数据级**：
        - Counterfactual Data Augmentation（CDA），做性别/种族词的对调，或通过额外收集目标数据来平衡分布。
    - **模型级**：
        - 对抗训练（adversarial training）隐藏或对冲对受保护属性的利用。
        - 更改损失函数（加正则项）或使用知识蒸馏等方式减少偏见。
        - 训练性别中立的词嵌入（如 GN-GloVe）。
    - **推断级**：
        - 直接在推断时修改输出，如利用对比学习矫正（FairFil），或通过触发词（trigger phrase）减少偏见输出。
        - 对静态/动态词向量进行后处理矢量正交投影去除偏见维度（Bolukbasi 等方法）。

3.2 **视觉模型中的偏见（Bias in Vision）**

- 3.2.1 **衡量与分析**
    - **多发现象**：
        - 人脸识别中，肤色较深群体更易被误判或被视作非人类标签（“猩猩”事件等）。
        - 情感识别中，对黑人更易标注为“愤怒”，对女性、跨性别或其它少数群体的识别准确度偏低。
    - **衡量方式**：
        - 内在指标：iEAT 等，用图像嵌入去做类似 WEAT 的关联度测试。
        - 外在指标：观察在分类、目标检测、人脸识别等任务中，不同群体的误差率差异。
- 3.2.2 **缓解方法**
    - **数据级**：
        - 通过 GAN 或其它生成对抗网络来生成少数群体的合成图像，实现数据增广；或额外收集平衡数据集。
    - **模型级**：
        - 处理训练数据不平衡（up-sampling 或加权等），或使用对抗/正则方法将种族性别信息尽量剥离。
        - “公平即盲”思路（Fairness Through Blindness） vs. “公平需感知”思路（Fairness Through Awareness）；也可将模型拆分为不同群体子分类器以提高公平性。
    - **推断级**：
        - 微调输出决策边界；后处理校准；多精度校正等。

---

# 4. 多模态（Vision & Language）模型中的偏见（Bias in Vision & Language Modeling）

4.1 **多模态偏见面临的挑战**

- 图像与文本的表达形式不同，文本可描述更多抽象特征，而图像只提供可见信息。
- 构建跨模态对照或“对抗样本”（如仅变换性别或种族信息）难度大。
- VL 数据集往往包含带有偏见的标注（如带有刻板印象的图像标题），并且真实世界数据分布不均衡。
- 模型是否准确捕捉了敏感属性并避免误用，也是一大难点。
```
### 1. **文本与图像表达能力的差异**

- **可见与不可见属性**：文本可以表达可见和不可见的属性（如智力、能力），而图像仅能客观表达可见属性，这可能导致刻板印象的放大。
- **视觉细节的表达**：图像比文本更能展现可见属性，而文本通常会省略细节，如性别信息通常通过代词表达，使得 NLP 偏见研究更关注性别问题。

### 2. **创建反事实数据的困难**

- 在 NLP 中，反事实数据易于构造（如替换 "woman" 为 "man"），保证句子仍然语法正确且意义保持一致。
- 在 CV 中，仅改变图像的性别属性并不容易，现有方法（如对抗性攻击生成反事实样本）仍存在争议，且难以控制背景等混杂因素。

### 3. **从图像中去除敏感属性的困难**

- 在文本中，去除敏感属性相对简单（如 "weak girl" 变为 "weak person"）。
- 在图像中，使用遮盖或模糊技术来去除敏感信息可能引入虚假相关性，且对某些 VL 任务（如图像生成）不适用。

### 4. **VL 数据集中的偏见分布**

- 互联网收集的图像在种族和性别上存在偏见，职业相关的检索结果往往夸大性别比例（如 "nurse" 主要是女性，"CEO" 主要是男性）。
- 标注者可能无意识地强化社会刻板印象：
    - **语言偏见**：如果性别与传统角色不符（如女性外科医生、男性护士），性别信息更容易被强调。
    - **不当推断**：标注者可能基于刻板印象进行额外推断（如图像中的 "经理-下属" 关系）。
    - **刻板印象错误**：错误使用性别词汇（如用 "man" 形容一名女性）。

### 5. **理解敏感属性的困难**

- VL 模型容易受到上下文影响，导致错误归因（如骑自行车、踢足球等活动被认为更符合男性）。
- VL-BERT 比 BERT 具有更强的性别偏见，例如更倾向于将女性与钱包联系在一起，将男性与公文包联系在一起。
- 语言先验可能会导致过拟合，如图像字幕模型错误描述 “穿西装打领带的男人”，即使图像中没有这样的视觉元素。

### 6. **生成无偏推断的困难**

- **错误关联**：如 CLIP 模型将黑人更可能误分类为非人类类别（黑猩猩、大猩猩），或将男性误分类为犯罪相关类别。
- **刻板印象关联**：
    - 生成性模型（如 Stable Diffusion）在性别/种族中立提示下仍倾向生成特定性别或肤色（如 "nurse" 全是女性，"salesperson" 全是男性）。
- **群体稳健性**：模型在不同人口群体上的表现不均衡，如 CLIP 的零样本分类在少数群体上表现较差。
```

4.2 **VL 模型主要架构概览**

- 4.2.1 **统一式编码器（Fusion VL Encoder）**
    - 早期如 LXMERT 等，用单一 Transformer 结构对图像特征（常由检测器提取）与文本做融合。
- 4.2.2 **双编码器（Dual-Stream Encoders）**
    - 如 CLIP 等，分别对图像和文本做编码，在输出层做对比学习以对齐视觉与语言语义空间。
- 4.2.3 **Encoder-Decoder 结构**
    - 用统一或分开的编码器、解码器做多模态预训练，可支持生成任务（如图文生成、图像生成等）。

4.3 **多模态偏见评估（Bias Evaluation）**

- 4.3.1 **内在指标**
    - Grounded-WEAT/Grounded-SEAT 等，将传统词嵌入测偏移到多模态场景测 VL 嵌入中的偏见。
- 4.3.2 **外在指标**
    - **图像描述（Captioning）**：统计生成的标题中错误的性别用词、或比例失衡情况。
    - **图文匹配/检索（Image Retrieval/Ranking）**：对性别或种族中性查询，看模型检索出的结果是否在受保护属性上显著倾斜。
    - **零样本分类（Zero-shot Classification）**：测试 CLIP 等在识别不同种族/性别图像时是否有错误率差异，或负面标签歧视性分配。
    - **文本-图像生成（Text-to-Image Generation）**：用带有职业或角色的中性提示词，看生成的结果是否在性别、种族等方面存在严重偏差。

4.4 **多模态偏见缓解方法（Bias Mitigation）**

- 4.4.1 **模型级（In-processing）**
    - Confusion & Confidence Loss（在图像描述中鼓励模型注意图中真正的性别线索，而非刻板语境）。
    - 对比学习/对比损失，用对抗或余差学习等方法减少对敏感属性的过度依赖。
    - 平衡采样策略，对训练数据做性别/种族上的加权或均匀选取。
    - Prompt Tuning（在文本提示中嵌入可学习的向量，并加上对抗目标，用以降低不平等表示）。
- 4.4.2 **推断级（Post-processing）**
    - Feature Clipping：剪除特征中最相关于性别/种族的那部分维度，但可能牺牲精度。
    - Prompt/Instruction Engineering：在推断阶段为扩散模型或生成模型添加“公平指令”，对结果再次重排或进行偏置投影。
    - Bias Vector Projection：将文本嵌入投影到去偏置方向，对双编码器模型能带来一定的后期修正。

4.5 **未来方向（Future Direction）**

- 4.5.1 **偏见评估与分析**
    - 探讨不同预训练架构对偏见的影响；在双编码器中做更细粒度的单模态 vs. 多模态偏见测量。
    - 考虑提示词变动带来的评测波动，提升评测的稳健性。
- 4.5.2 **偏见缓解方法探索**
    - 进一步发展低成本（后处理/轻量式微调）的去偏方法，以适配大规模预训练模型。
    - 同时处理多种敏感属性（性别、种族、年龄等）的联合去偏，在不降低主任务性能的前提下寻求更稳健的解决方案。

---

# 5. 总结（Conclusion）

- 本文调查了自然语言处理、计算机视觉以及新兴多模态（视觉-语言）模型在社会偏见问题上的研究进展。
- NLP 与 CV 在偏见检测和缓解方面研究较多，但多模态领域需要更多投入，尤其在数据构建、偏见测量指标设计及有效去偏技术开发方面。
- 通过总结三大领域的偏见评估与干预技术，希望为研究者在多模态环境中识别、理解和减轻社会偏见提供有益指引。
- 随着大规模多模态模型的广泛应用，保障 AI 系统公平性与合规性的任务将持续重要，期待更多跨学科的探讨与合作。

---

**以上层次化总结**涵盖了论文从概念与术语、衡量指标、缓解方法，到单模态（NLP、CV）与多模态（VL）领域偏见分析的主要框架，并提供了若干未来工作方向。希望对你快速了解和回顾该综述的核心内容有所帮助。