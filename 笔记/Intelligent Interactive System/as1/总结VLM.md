
| 论文名                                                                                                                                                                                                                                                    | 引用                              | 提出的偏见                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 是否de-bias                                                                                                                                                                                                                                                                                                                                                                                                                          | 是否衡量bias                                                                                                                                                                                                                                                                                                                                                                    | 是否有新数据集                                                                                                                                                                                                                                                                                                 | 主要贡献                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| [Identifying Implicit Social Biases in Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0cc81-9130-8001-b590-e856c345a24c)                                                                      | hamidieh2024identifying         | - “恐怖分子（terrorist）” → 主要检索到 **中东男性**。<br>- “家庭主妇（homemaker）” → 主要检索到 **印度女性**。<br>- “罪犯（felon）” → 主要检索到 **黑人**。<br>- “英俊（handsome）” → 主要检索到 **白人男性**。<br>- “野心勃勃（ambitious）” → 主要与 **男性** 相关，而“专横（bossy）”主要与 **女性** 相关。                                                                                                                                                                                                                                                                                                                                                       | 没有提出新的方法，仅分析了现有方法的局限性。<br><br>- 研究发现，已有的去偏见方法（如 DebiasCLIP）虽然减少了性别偏见，但加剧了种族偏见。<br>- 强调了“去偏见方法需要交叉评估，否则会像打地鼠一样，消除一种偏见却增加另一种偏见”。                                                                                                                                                                                                                                                                                                     | 是的，提出了一种 **C-ASC 指标**（Caption-Association Similarity Comparison）。<br><br>- 计算 **文本-图像相似度的均值差值**，用于量化某一描述与某一人群的关联程度。<br>- **相比 WEAT**：<br>    - C-ASC 适用于视觉-语言模型，而 WEAT 主要用于文本嵌入。<br>    - C-ASC 适用于大规模数据的自动化分析，而 WEAT 需要人工筛选词汇。                                                                                                                                             | - 主要使用现有的 **FairFace** 数据集进行分析。<br>- 但强调了 **训练数据的透明度问题**，建议未来研究者在数据层面进行去偏见处理。                                                                                                                                                                                                                           | **“提出 So-B-IT 偏见分类体系，发现 CLIP 模型在种族、性别等方面存在严重偏见，并分析其来源及去偏见方法的局限性。”**                                           |
| [A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0f64f-3364-8001-b45c-35a642a06943)                                    | berg2022prompt                  | - CLIP在性别相关文本（如“聪明的人”）上有偏见，偏向于男性。<br>- CLIP会将某些族裔（如黑人）错误分类为“犯罪分子”或“非人类”。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 是的模型层面，提出了一种**可学习提示 + 对抗去偏 + ITC联合训练**方法，属于**模型层面（In-processing）**的方法。                                                                                                                                                                                                                                                                                                                                                             | 是的，提出使用**MaxSkew@k 和 NDKL**来替代WEAT测量VL模型偏见。                                                                                                                                                                                                                                                                                                                                 | 没有。主要使用FairFace和UTKFace进行实验。                                                                                                                                                                                                                                                                            | 请问这篇paper是否比较了各个模型的偏见程度以及偏见模式？                                                                                |
| [A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0f7f0-9f14-8001-9253-77f88277d497)                                               | jung2024unified                 | - **零样本分类**：职业分类中，“木匠”更容易被识别为男性。<br>- **文本-图像检索**：“穿西装的人”更可能检索到男性图片。<br>- **图像字幕生成**：“运动员”更常被描述为男性。<br>- **文本-图像生成**：“程序员”多被生成为男性，即使使用中性提示词。                                                                                                                                                                                                                                                                                                                                                                                                                                  | 是的，提出了**选择性特征填充（SFID）**，属于**模型层面（In-processing）**的方法。<br>适用范围更广：适用于多种任务和模态<br>无需重新训练，计算成本低<br>保持模型语义完整性，避免误删重要信息<br>适用于更复杂的敏感属性                                                                                                                                                                                                                                                                                                    | 主要采用已有的评估方法（如 ΔDP, Skew@M, MRC 等）。                                                                                                                                                                                                                                                                                                                                          | 使用了现有数据集（FACET, FairFace, Bias-in-Bios等）。                                                                                                                                                                                                                                                               |                                                                                                               |
| [A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0f9bf-74fc-8001-8d94-1f5158f8afb1)                                             | sathe2024unified                | - **文本-图像（T2I）偏见**：<br>    <br>    - **当输入文本“某个职业的人”**，大多数VLM会生成**白人男性**。<br>    - DALL-E-3和SDXL在职业角色上表现出性别不平衡，如“科学家”多为男性，“护士”多为女性。<br>- **图像-文本（I2T）偏见**：<br>    <br>    - **给定一张中立职业图像**（机器人执行任务），VLM仍然倾向于将其描述为某个性别（通常是男性）。                                                                                                                                                                                                                                                                                                                                                 | **没有直接提出新的偏见缓解方法**，但提供了**偏见评估的新框架和去偏见数据集**，可用于未来研究。                                                                                                                                                                                                                                                                                                                                                                                | **是的，提出了新的度量指标——“Neutrality”**：<br><br>- 计算模型在中立输入上的“无偏见”预测比例。<br>- 解决了传统“平均性别偏见（AG）”指标的不稳定性。                                                                                                                                                                                                                                                                               | **是的，提出了新的数据集**：<br><br>- **基于行动的职业描述数据集**，避免直接使用人物外貌信息。<br>- **自动化生成无偏见文本-图像对**，确保模型无法通过文本或视觉线索推断性别、种族、年龄。                                                                                                                                                                                             |                                                                                                               |
| [American == White in Multimodal Language-and-Image AI](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0fb40-6ba8-8001-bedd-7844657c459e)                                                                             | wolfe2022american               | - 视觉问答任务：<br>    - 96.7% 白人被认定为美国人，而仅 2.8% 亚裔被认定为美国人。<br>    - 53.2% 亚裔被错误地认定为住在中国，而所有白人都被认为住在美国。<br>- 图像字幕任务：<br>    - 36.7% 的亚裔照片中，模型主动标注了种族，但对白人从不标注。<br>- 合成图像任务：<br>    - VQGAN-CLIP 在生成“美国人”时，会自动将所有种族的肤色变白。                                                                                                                                                                                                                                                                                                                                                            | 没有提出新的方法，只是对现有方法进行了分析。                                                                                                                                                                                                                                                                                                                                                                                                             | - **视觉语义嵌入测试（EAT 和 SC-EAT）**:<br>    - 通过测量图像与不同概念的语义相似度，评估 AI 对种族身份的隐含偏见。<br>- **下游任务测试**:<br>    - 通过视觉问答、图像字幕、合成图像等任务，进一步分析 AI 偏见如何影响实际应用。                                                                                                                                                                                                                               | 没有提出新的数据集，主要使用了 **Chicago Face Database (CFD)** 进行分析。                                                                                                                                                                                                                                                   | 该研究系统性地揭示了多模态 AI（CLIP、SLIP、BLIP）如何学习和放大“美国人 = 白人”的隐性偏见，并通过嵌入测试和下游任务分析其对 AI 视觉语义和应用的影响。                        |
| [BiasDora: Exploring Hidden Biased Associations in Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0fc39-3e5c-8001-8c53-2bc74956cf8b)                                                          | raj2024biasdora                 | 发现了一些新的bias。- **性取向偏见**："同性恋者↔不道德"、"女同性恋者↔疯子"。<br>- **种族偏见**："意大利人↔绑架犯"、"犹太人↔小偷"、"黑人↔奴隶"。<br>- **外貌偏见**："胖人↔贪吃"、"丑陋的人↔尴尬"。<br>- **宗教偏见**："无神论者↔犯罪"、"穆斯林↔圣战者"。                                                                                                                                                                                                                                                                                                                                                                                                                 | - **没有提出新的偏见缓解方法**，但提供了一种新的检测框架。                                                                                                                                                                                                                                                                                                                                                                                                   | **是的**，论文提出了一种新的偏见评估框架，包括：<br><br>1. **统计显著性检测**（TF-IDF + p值）。<br>2. **情感分析**（DistilBERT）。<br>3. **毒性检测**（ROBERTA模型）。<br>4. **LLM评分**（Likert 评分）。                                                                                                                                                                                                                           | - **是的**，该论文发布了**Dora数据集**，收集了VLMs的偏见关联信息。                                                                                                                                                                                                                                                              | - 该论文提出了一种**全面的偏见探测框架**，揭示了**VLMs中的隐性偏见**，并发布了**Dora数据集**，为未来研究提供了系统性的偏见分析方法。                                 |
| [Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0fd9b-8a04-8001-bff5-2da632cd7c20)                                       | wolfe2024dataset                | - **可信度（Trustworthiness）偏见**<br>    <br>    - CLIP对可信度的预测与人类评分相关，但可信度是不可直接从面部观察的，这表明模型继承了社会刻板印象。<br>    - 这种偏见在2B数据规模的模型中更加明显。<br>- **性取向（Sexuality）偏见**<br>    <br>    - CLIP对“同性恋”属性的预测与人类评分一致，但性取向无法从面部特征推断，说明模型学到了社会刻板印象。<br>- **Stable Diffusion的种族偏见**<br>    <br>    - 生成的“白人”比“黑人”更容易被分类为“具备领导力、被选举能力高、吸引力高、快乐”。<br>    - 生成的“黑人”更容易被分类为“自由派、更重（胖）”。                                                                                                                                                                                                                  | **没有**，本文主要是分析和测量偏见，而未提出新的缓解方法。                                                                                                                                                                                                                                                                                                                                                                                                    | 是的，本文提出了以下新的衡量方法：<br><br>- **CAT（Correlated Attribute Test）**<br>    - 计算CLIP模型内部两个属性之间的相关性，以分析模型中不同偏见的结构。<br>- **Frobenius Inner Product**<br>    - 计算模型学习到的偏见结构与人类偏见结构的相似性，用于衡量模型偏见的整体结构性偏差。<br>- **子空间投影（Subspace Projection）**<br>    - 在Stable Diffusion生成的图像上应用子空间投影方法，以量化其学习到的面部印象偏见。                                                                                | **没有**，但它使用了OMI数据集来研究偏见，并强调了数据集质量对模型公平性的影响。                                                                                                                                                                                                                                                             | 本研究证明了CLIP等视觉-语言模型会学习人类面部印象偏见，并发现数据规模和社会一致性是影响偏见学习的重要因素，同时揭示了这些偏见在文本-图像生成模型中的延续和放大。                           |
| [DeAR: Debiasing Vision-Language Models With Additive Residuals](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a0fe9f-69b4-8001-b08c-f442e4f81857)                                                                    | seth2023dear                    | - **职业偏见**:<br>    - CLIP 在“医生”和“护士”文本描述时，更多地将“医生”与男性匹配，而将“护士”与女性匹配。<br>- **负面刻板印象**:<br>    - 在 PATA 数据集中，模型更容易将“遵纪守法的公民”匹配到白人图像，而负面描述（如犯罪分子）更容易匹配到黑人或其他少数族裔。<br>- **对象检测偏见**:<br>    - 在开放词汇物体检测任务中，CLIP 可能只会检测男性为“医生”，而女性则更可能被标注为“护士”。<br>- **视觉关注偏见**:<br>    - 在职业相关任务中，未经去偏的 CLIP 主要关注人脸，而去偏后则更多关注实际的职业工具（如医生的听诊器）。                                                                                                                                                                                                                                                     | 是的，论文提出了 **DeAR（Debiasing with Additive Residuals）** 方法：<br><br>- **属于模型层面的（In-processing）方法**，在视觉编码器的图像表示上添加残差调整。<br>- **优势**:<br>    - **轻量级**: 仅需训练一个小型残差模块，无需重新训练整个 VLM。<br>    - **兼容性强**: 可与任何 VLM（如 CLIP、FLAVA、BLIP）结合使用。<br>    - **保留模型性能**: 只对视觉编码器进行调整，尽可能保持零样本能力。                                                                                                                                                      | 论文使用已有的偏见衡量指标，如 **MaxSkew / MinSkew**，并未引入新的度量方法。                                                                                                                                                                                                                                                                                                                           | 是的，论文引入了 **PATA（Protected Attribute Tag Association）数据集**：<br><br>- **特点**:<br>    - 由 **4,934 张带场景上下文的人物图像** 组成。<br>    - 场景多样，包括职业、生活场景等，以检测不同情境下的偏见。<br>    - 提供 **正面（positive）和负面（negative）文本标签**，可以测试模型在不同语境下的偏见程度。<br>- **优势**:<br>    - 现有数据集（如 FairFace）仅包含人脸，缺乏上下文，而 PATA 提供了场景信息，使偏见检测更具现实意义。 | 论文提出了一种基于可加性残差（DeAR）的去偏方法，通过调整视觉表示减少 VLMs 的社会偏见，并引入了 PATA 数据集，以提供更具上下文信息的偏见测试环境。                              |
| [Debiasing Vision-Language Models via Biased Prompts](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a10025-c798-8001-b06d-66a43d9718b4)                                                                               | chuang2023debiasing             | - **职业偏见**：例如，“医生”更倾向于被模型识别为男性，而“护士”更倾向于女性<br>- **背景偏见**：在 Waterbird 数据集中，水鸟的分类受到背景（水/陆地）的影响<br>- **生成模型偏见**：Stable Diffusion 生成的“消防员”或“工程师”主要为男性                                                                                                                                                                                                                                                                                                                                                                                                                             | 是的，提出了一种基于文本嵌入投影的方法：<br><br>- **属于推理阶段（Post-processing）** 方法<br>- **方法核心**：<br>    - 通过正交投影移除偏见方向<br>    - 通过校准投影矩阵提高去偏的稳定性<br>- **优势**：<br>    - 无需额外训练、数据或标签<br>    - 计算开销小，适用于大规模模型<br>    - 适用于判别和生成模型                                                                                                                                                                                                                         | 是的，提出了一种基于提示词嵌入的方法：<br><br>- 计算类目嵌入与偏见方向的余弦相似度<br>- 用 MaxSkew 量化文本-图像检索的公平性<br>- 通过嵌入差异衡量生成模型的偏见                                                                                                                                                                                                                                                                            | 没有，本文使用已有数据集（Waterbird、CelebA、FairFace）进行实验                                                                                                                                                                                                                                                             | 本文提出了一种无需训练、数据或标签的通用去偏方法，基于文本嵌入投影，有效减少判别和生成模型中的偏见。                                                            |
| [Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a147a7-14f8-8001-9629-d890fe9cc676)                                              | cabello2023evaluating           | 是的，文中提供了一些偏见放大的实例：<br><br>- **视觉问答 (VQA) 任务**：<br>    - 偏见放大案例：模型更倾向于在“某人在滑滑板”这类问题中预测“男”而不是“女”。<br>- **图像-文本检索任务 (Flickr30K)**：<br>    - 偏见放大案例：性别中立模型在检索任务中仍然更倾向于匹配**男性图像**，即在查询“某人正在工作”时，更可能检索到男性的图片。<br>- **内在偏见实验**：<br>    - 统计结果表明，模型在掩码语言建模任务中倾向于使用性别明确的术语，而非性别中立术语。                                                                                                                                                                                                                                                                                                   | 是的，作者提出了一种**数据层面的偏见缓解方法（Pre-processing）**：<br><br>- **方法：额外在性别中立数据上进行预训练**，即：<br>    - **修改文本数据**，将所有性别相关词汇替换为性别中立术语（如 “man” → “person”）。<br>    - **继续预训练一个 epoch**，让模型适应新的数据分布。<br>- **优势：**<br>    - 该方法**不会显著影响模型的任务性能**，即使模型的偏见有所降低，任务准确率仍然保持较高水平。<br>    - 相较于模型层面或推理层面的方法，该方法**操作简单**，仅需对数据进行预处理，并进行少量额外训练。                                                                                                                 | 没有提出新的衡量方法，而是使用了已有的 **BiasAmpA→T** 指标来测量偏见放大程度。                                                                                                                                                                                                                                                                                                                             | 没有推出新的数据集，而是**修改了现有数据集**（如 COCO、CC3M）来创建性别中立版本。                                                                                                                                                                                                                                                         | 本研究分析了**视觉-语言模型中的性别偏见放大效应**，并提出了一种**基于性别中立数据的额外预训练方法**来减少偏见，同时保持模型的任务性能和公平性。                                  |
| [Fair Federated Learning with Biased Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14876-56e4-8001-80f1-0090ffbe6671)                                                                        | zeng2024fair                    | - **是的，该论文通过 CLIP 在笑容检测任务中的实验，展示了 VLM 存在的偏见。**<br>- **示例：**<br>    - CLIP 预训练数据包含性别偏见：在“笑容检测”任务中，模型更倾向于将女性分类为“微笑”，将男性分类为“不微笑”。<br>    - 由于 FL 具有数据异质性，局部数据的偏见会放大 CLIP 的这种性别偏见，最终导致全局模型不公平。                                                                                                                                                                                                                                                                                                                                                                                    | - **是的，该论文提出了 Fair Federated Deep Visual Prompting (FF-DVP)。**<br>- **方法类型：** **模型层面的 (In-processing)**<br>- **核心方法：**<br>    - 通过公平感知的深度视觉提示 (Fairness-aware Deep Visual Prompting, DVP)，在 VLM 的视觉特征中去除人口属性相关信息。<br>    - 通过模态融合分类头 (Modality-fused Classification Heads) 进行公平约束优化。<br>- **优势：**<br>    - FF-DVP **不需要修改 CLIP 的权重**，仅通过插入轻量级提示 (Prompt) 实现去偏，适用于 FL 场景。<br>    - **低通信成本**，适用于 FL 场景中的分布式训练。                      | #### **这篇 paper 是否提出了新的原创方法来衡量 VLM 的偏见？**<br><br>- **没有，该论文采用了已有的公平性指标来衡量 VLM 偏见，包括：**<br>    - **Demographic Parity (Φ_demo)：** 各人口组的预测概率是否均匀。<br>    - **Equalized Odds (Φ_eq)：** 各人口组的误分类率是否一致。<br>    - **Accuracy Parity (Φ_A)：** 各人口组的准确率是否一致。                                                                                                                        | #### **这篇 paper 是否推出了新的数据集来削弱 VLM 的偏见？**<br><br>- **没有，该论文使用的是已有的 CelebA 和 FairFace 数据集，并未提出新的数据集。**                                                                                                                                                                                                    | 提出了 FF-DVP 框架，首次研究 VLM 在 FL 场景中的偏见，并通过公平感知的深度视觉提示 (DVP) 结合模态融合分类头，成功降低偏见，提高 FL 模型的公平性。                        |
| [Fairness in AI Systems: Mitigating gender bias from language-vision models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14909-0e94-8001-a5f9-0bde3339b448)                                                        | aggarwal2023fairness            | - **数据层面的偏见**：<br>    - MS-COCO 数据集中，某些活动（如“修理”、“教练”）更常与男性相关，而“购物”、“洗衣”更常与女性相关。<br>    - 数据标注者倾向于根据活动猜测性别，如骑摩托车的未指明性别人被标注为“man”。<br>- **模型层面的偏见**：<br>    - Google Translate 在翻译土耳其语时自动分配性别，如“doctor”翻译为“男性”，“nurse”翻译为“女性”。<br>    - 图像字幕模型在某些情况下，即使图像中是女性，仍然会标注为“man”，尤其是在涉及计算机的场景中。                                                                                                                                                                                                                                                                                        | 是的，提出了一种新的方法 **Show, Attend and Identify（SAI）**，其属于 **数据层面（Pre-processing）+ 模型层面（In-processing）** 结合的方法：<br><br>- **数据层面（Pre-processing）**: 通过将数据中的性别词替换为“person”或“people”，防止模型学习刻板印象。<br>- **模型层面（In-processing）**: 采用一个独立的 CNN 进行性别识别，避免模型直接依赖数据中的性别信息。<br>- **相比其他方法的优势**：<br>    - **相比数据再平衡方法**: 无需手动调整数据分布，适用于更多任务。<br>    - **相比后处理方法**: 直接在模型架构中解决偏见，而非事后修正，提高模型整体一致性。                                                     | 没有。论文主要关注如何**缓解**偏见，而没有提出新的偏见衡量方法。                                                                                                                                                                                                                                                                                                                                          | 是的，提出了三个子数据集（基于 MS-COCO），但它们是现有数据集的子集，而非全新数据集：<br><br>1. **MSCOCO Gender Confident**：确保数据中的性别信息是一致的。<br>2. **MSCOCO Human**：包含所有带有人类的图像，用于评估人物相关任务。<br>3. **MSCOCO Nature**：包含没有人类的图像，用于评估非人物任务。                                                                                                        | **提出了一种新的 VLM 去偏见方法（Show, Attend and Identify），通过数据预处理和模型架构改进减少性别偏见，并使用 MSCOCO 数据集构建了子数据集来评估方法的有效性。**         |
| [GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a149c8-4828-8001-89ca-d3eeddd3097f)                                                      | abdollahi2024gabinsight         | 是的，文章举了一些例子：<br><br>- 在“**修理设备**”的场景中：<br>    - **如果画面只有女性**，模型能够正确识别活动并生成正确的描述。<br>    - **如果画面中同时有男性和女性**，即使女性正在修理设备，模型仍然倾向于认为**男性是执行者**。<br>- **文本编码器实验**显示：<br>    - 在输入“**一个人在修理设备**”的情况下，模型更倾向于匹配“**一个男人在修理设备**”，而不是“**一个女人在修理设备**”。                                                                                                                                                                                                                                                                                                                                    | 没有提出新的偏见缓解方法，而是**分析了现有的偏见缓解方法**，包括：<br><br>- **数据层面（Pre-processing）**：<br>    - 通过**生成对抗样本**（Counterfactual Data）来减少数据中的刻板印象。<br>- **模型层面（In-processing）**：<br>    - **正交投影方法**（Orthogonal Projection）来消除偏见信息。<br>    - **添加残差表示**（Additive Residual Representation）来校正偏见。<br>- **推理层面（Post-processing）**：<br>    - **剪裁高互信息特征**（Clipping High-Mutual-Information Features）以减少偏见传播。<br><br>**文章没有提出新方法，而是回顾了已有的解决方案，并指出每种方法的局限性**。 | 没有。文章使用了**传统的检索任务（Text-to-Image和Image-to-Text）**来测量偏见，并分析了**文本编码器的匹配得分**。                                                                                                                                                                                                                                                                                                   | **是的，提出了GAB（Gender-Activity Binding）数据集**，该数据集：<br><br>- **包含5500张AI生成的图片**，展示不同性别执行各种活动的场景。<br>- **涵盖三种类型的活动**：<br>    - **刻板印象活动**（如“男性修车”、“女性化妆”）。<br>    - **日常活动**（如“男性编程”、“女性烘焙”）。<br>    - **LAION-400M中的性别偏见活动**（如“男性喝酒”、“女性挑选衣服”）。<br>- 通过**DALL-E 3生成，并进行质量控制**，确保数据的多样性和真实性。                 | **提出了“性别-活动绑定偏见”（GAB Bias）这一新问题，并构建了GAB数据集，系统评估VLMs在文本-图像检索任务中的偏见。**                                          |
| [Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14baf-7450-8001-8f61-c6a28e2d12d8)    | weng2024images                  | 是的，论文通过实验展示了 VLM 存在的偏见，具体包括：<br><br>- **错误关联物体与性别**：<br>    - **室内物体（如宠物、家具）更倾向于关联女性**。<br>    - **室外物体（如汽车、飞机）更倾向于关联男性**。<br>    - **举例**：<br>        - 在 **PASCAL-SENTENCE** 数据集中，女性图像的 **False Positive Rate（FPR）** 在宠物、家具等类别上更高，而男性图像在交通工具类别上FPR更高。<br>        - 在 **MSCOCO** 数据集中，类似趋势也得到验证。<br>- **偏见度量指标**：<br>    - **BIAS_VL** 指标显示，模型在数据集上推理时的性别偏见值远高于随机分割数据集的偏见值，证明了 VLMs 在训练过程中放大了社会偏见。                                                                                                                                                                           | 是的，本文提出了一种新的**模型层面（In-processing）**方法来缓解VLM偏见：<br><br>- **方法**：<br>    - **图像编码器消除性别特征**：<br>        - 使用 **MTCNN（面部检测）** + **MobileNet（性别分类）** 识别并替换人脸区域，模糊性别特征。<br>    - **文本编码器消除性别特征**：<br>        - 通过替换性别词并计算其反向性别词特征均值，减少文本的性别信息影响。<br>- **优势**：<br>    - **比现有方法更有效**：相比于仅修改输入或后处理结果，该方法能在模型内部消除性别特征，偏见减少幅度更大（22.03% vs. 7.8%）。<br>    - **计算代价小**：集成轻量级检测模型（MTCNN + MobileNet），对推理性能影响较小（模型性能下降 <0.5%）。                   | 是的，本文提出了一种新的偏见衡量方法：<br><br>- **BIAS_VL 指标**：计算不同性别在分类错误（False Positive Rate, FPR）上的差异，并用 **因果中介分析（CMA）** 来分解偏见的来源（图像 vs. 文本）。<br>- **优势**：<br>    - **能追踪偏见的来源**（图像、文本、融合模块）。<br>    - **相比传统方法更精细**，能量化偏见的直接和间接效应。                                                                                                                                                         | 没有，本文没有提出新的数据集，而是在 **MSCOCO 和 PASCAL-SENTENCE** 数据集上进行了实验。                                                                                                                                                                                                                                              | 本文提出了一种**因果中介分析（CMA）**框架，以定量分析VLM偏见的生成与传播路径，发现**图像编码器是偏见的主要来源**，并提出了一种**基于模糊性别特征的偏见消除方法**，在不影响模型性能的情况下有效减少偏见。 |
| [Joint Vision-Language Social Bias Removal for CLIP](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14cb7-687c-8001-9552-747a29e1908c)                                                                                | zhang2024joint                  | - **职业-性别偏见**：<br>    - CLIP 更倾向于将“职业”与“男性”关联。<br>- **性别-科学偏见**：<br>    - CLIP 在文本嵌入中倾向于将“科学”与“男性”联系更紧密。<br>- **种族偏见**：<br>    - 通过 FACET 数据集，发现 CLIP 在种族识别上存在不公平性。                                                                                                                                                                                                                                                                                                                                                                                                           | - **是的，提出了一种新的模型层面 (In-processing) 方法。**<br>    <br>- **核心方法：**<br>    <br>    - **双模态偏见对齐**：先对齐图像和文本的偏见信息。<br>    - **反事实去偏**：利用“反事实对”来引导去偏。<br>- **相比其他方法的优势：**<br>    <br>    - 解决了现有方法“过度去偏”导致的 V-L 对齐能力下降问题。<br>    - 兼容多种社会偏见（性别、年龄、种族）。<br>    - 在多种数据集上泛化性更好。                                                                                                                                                              | 是的，提出了 **新的公平性评估指标 ABLE**，用于平衡去偏效果与 V-L 对齐能力。                                                                                                                                                                                                                                                                                                                               | **没有**，但使用了多个现有数据集（FairFace、UTKFace、FACET）进行评估。                                                                                                                                                                                                                                                         | **提出了一种新颖的 V-L 去偏方法，通过双模态偏见对齐和反事实去偏，有效减少社会偏见，同时保持 V-L 对齐能力，并引入新的评估指标 ABLE 进行全面评估。**                           |
| [Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14d78-824c-8001-8a9f-86f7ec202da2)                                                   | sasse2024mapping                | - **OccupationBias 数据集**：VLMs会将特定职业与特定性别或种族联系起来。<br>- **PAIRS 数据集**：VLMs在相同场景下，对不同种族或性别的描述会有所不同。<br>- **VisoGender 数据集**：在医生-患者场景下，模型更倾向于将“医生”解析为男性。                                                                                                                                                                                                                                                                                                                                                                                                                          | - **没有提出新的缓解方法**，但强调了改进数据集的重要性，以减少模型仅依赖文本的倾向。                                                                                                                                                                                                                                                                                                                                                                                      | **提出了VisoGender Adversarial**：该数据集故意提供错误的文本描述，以测试模型是否能够依赖视觉信息而非文本。                                                                                                                                                                                                                                                                                                          | **没有推出新的数据集**，但提出了对现有数据集（VisoGender）的改进建议，使其更具挑战性。                                                                                                                                                                                                                                                      | 本文系统分析了VLMs的偏见，并评估了现有数据集在测量偏见方面的有效性，发现当前的场景数据集和代词解析数据集存在局限性，强调了改进数据集设计的重要性。                                   |
| [MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14e4e-5f50-8001-a90a-6c2f8934b0da)       | janghorbani2023multimodal       | 是的，paper 通过实验明确展示了 VLM 存在偏见，具体例子包括：<br><br>- **宗教**：<br>    - **伊斯兰教、犹太教**常被 CLIP 关联到负面词汇，如“恐怖主义（terrorist）”、“极端主义（extremist）”等。<br>    - **基督教、佛教**则多被关联到正面词汇，如“和平（peace）”、“祝福（blessing）”等。<br>- **国籍**：<br>    - **阿拉伯人**和**伊斯兰教**的偏见高度相似，都被 CLIP 关联到“贫穷（impoverished）”和“恐怖主义（terrorism）”。<br>    - **墨西哥人**被频繁关联到“非法移民（illegal）”和“贪婪（greed）”。<br>    - **美国人**则常与“成功（success）”、“富有（wealthy）”等正面词汇相关。<br>- **性取向**：<br>    - LGBTQ 群体被 CLIP 关联到“冒犯（offending）”、“粗俗（vulgar）”、“变态（perverse）”等负面词汇。<br>- **残疾**：<br>    - 残疾人群被 CLIP 关联到负面情绪，如“痛苦（suffering）”、“悲惨（miserable）”。 | 是的，paper 提出了一种新的**后处理（Post-processing）**去偏方法：<br><br>- **核心方法**：<br>    - 通过计算**模型嵌入的不同维度对偏见的贡献**，去除高偏见的特征维度。<br>- **相较于其他方法的优势**：<br>    - **无需重新训练模型**，对大规模预训练模型（如 CLIP）特别有用。<br>    - **计算成本较低**，仅需处理嵌入空间，不影响推理速度。<br>    - **保持模型性能**，去偏后分类准确率下降极小（仅 1.1%）。                                                                                                                                                                    | 没有。衡量偏见的方法基于**Caliskan 偏见评分**和**零样本分类实验**，并未引入新的衡量指标。                                                                                                                                                                                                                                                                                                                       | 是的，paper 提出了 **MMBias 数据集**：<br><br>- **包含 3,500 张图片 + 350 条文本短语**。<br>- **涵盖 14 个群体（宗教、国籍、性取向、残疾）**，填补了现有数据集的空白。<br>- **可用于 VLM 偏见分析和去偏方法评估**。                                                                                                                                                         | **提出 MMBias 数据集，评估主流 VLM 偏见，证明它们存在对特定群体的偏见，并提出高效的后处理去偏方法，在降低偏见的同时保持模型性能。**                                    |
| [Procedural Humans for Computer Vision](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a14f47-e160-8001-891f-db2c15d0d340)                                                                                             | hewitt2023procedural            | - **没有**，该论文主要关注的是通过 **合成数据** 生成多样化人体数据，以减少计算机视觉任务中的偏见，而未具体讨论 VLM（Vision-Language Model）中的偏见案例。                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | **没有直接讨论 VLM 的偏见问题**，但提出了一种 **基于合成数据** 的方法，旨在减少计算机视觉模型的偏见：<br><br>- 该方法主要属于 **数据层面的（Pre-processing）解决方案**，通过生成高质量、多样化的人体数据来提高模型的公平性。<br>- **优势**：<br>    - 可以精准控制数据中的性别、肤色、姿势、服装等因素，从而减少数据不均衡带来的偏见。<br>    - 相比于传统的 **数据增强** 或 **公平性约束**，这种方法提供了完全可控的数据生成机制。                                                                                                                                                                       | - **没有**，该论文的核心是生成合成数据，并未涉及 VLM 偏见的衡量方法。                                                                                                                                                                                                                                                                                                                                    | **推出了新的合成数据集**：<br><br>- 采用 **程序化方法（Procedural Generation）** 生成 10 万张 **带注释的人体图像数据集**。<br>- 该数据集包含 **不同性别、肤色、姿势、服装** 等多样性特征。<br>- 目的是增强计算机视觉模型的泛化能力，减少真实数据中的偏见。                                                                                                                                         | - 该论文提出了一种高保真的**人体合成数据生成管线**，结合 **面部、身体、手部模型**，并利用该数据提升计算机视觉任务（如关键点回归、模型拟合）的表现，**以减少数据偏见**。                   |
| [Run Like a Girl! Sports-Related Gender Bias in Language and Vision](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a15008-42e4-8001-83ca-4bb7854d530f)                                                                | harrison2023run                 | 是的，论文举了多个例子来证明视觉-语言模型（VLM）存在性别偏见：<br><br>1. **数据集中的性别比例失衡**：<br>    <br>    - **VisualGenome** 数据集中女性占比 **32.7%**，而现实世界中女性占比 **49.6%**。<br>    - **ManyNames** 数据集中女性占比 **47.6%**，但在运动相关图片中仅占 **30.2%**。<br>2. **人类命名数据中的偏见**：<br>    <br>    - 研究发现，在 ManyNames 数据集中，男性运动员的图片有 **46%** 被标注为"体育相关名称"（如“网球运动员”）。<br>    - 而女性运动员的图片只有 **35%** 被标注为"体育相关名称"，更多时候被标注为“女人”或“女孩”而非具体的运动身份。<br>3. **L&V 预测模型的偏见**：<br>    <br>    - 该模型学习并复现了人类的命名偏见：<br>        - 男性运动员更容易被识别为特定运动员（如“滑雪者”）。<br>        - 女性运动员更可能被识别为一般的女性身份（如“女人”）。                                              | - 论文主要是分析和测量偏见，而没有提出新的缓解方法。<br>- 论文提及未来可以在数据收集、数据均衡和模型训练方面采取措施，但未提出具体方案。                                                                                                                                                                                                                                                                                                                                                           | **是的，提出了一种新方法**：<br><br>- **通过分析人类命名数据，衡量L&V数据集中对女性运动员的认知偏见**。<br>- 具体方法：<br>    - 统计 ManyNames 数据集中涉及运动的图片，并计算 "体育相关名称" 在不同性别上的分布差异。<br>    - 通过 **logistic 回归分析** 计算不同性别的命名比例，发现男性运动员更容易被命名为具体运动员，而女性运动员更容易被命名为“女人”或“女孩”。<br>    - 训练一个 L&V 模型，并测试其是否复现了人类命名偏见。<br><br>**创新点**：<br><br>- 以 **命名方式** 为衡量指标，而非仅仅依赖于数据中的性别比例失衡。<br>- 结合 **人类行为分析 + 机器学习模型分析**，提供了更全面的偏见测量方式。 | - 论文主要分析现有数据集（ManyNames 和 VisualGenome）的偏见问题，并未提出新的数据集。                                                                                                                                                                                                                                                 | **该论文首次发现了人类命名中的性别偏见，并证明视觉-语言模型学习并复现了这一偏见，尤其是在女性运动员的身份识别上存在系统性不公正。**                                          |
| [Scaling for Fairness? Analyzing Model Size, Data Composition, and Multilinguality in Vision-Language Bias](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a150de-7934-8001-90b2-2235574889d8)                         | sahili2025scaling               | 是的，文章提供了多个例子来证明 VLM 存在偏见，包括：<br><br>- **犯罪标签偏见**：某些模型更容易将特定种族的面孔（如黑人、拉美裔）误分类为“犯罪分子”或“可疑人员”。<br>- **社会属性偏见**：<br>    - 在 Communion（社交亲和力）维度上，**白人更可能被标记为“可信”或“友善”**，而其他种族可能被标记为“冷漠”或“不值得信赖”。<br>    - 在 Agency（权力和能力）维度上，**男性更可能被标记为“强大”或“有影响力”**，而女性更可能被标记为“温顺”或“顺从”。<br>- **多语言偏见**：即使是语法上不强调性别的语言（如芬兰语、波斯语），仍然存在性别刻板印象，如女性更可能被描述为“友善”而非“有权力”。                                                                                                                                                                                                                               | **没有提出新的偏见缓解方法**，但强调了：<br><br>- **扩大数据规模并不能完全消除偏见**，数据质量和多样性比数据量更重要。<br>- **多语言训练可能转移或放大偏见**，不能被简单地认为是消除偏见的手段。<br>- **未来研究应重点关注数据构建的公平性**，而不是仅依赖模型扩展。                                                                                                                                                                                                                                                                              | **没有提出全新的衡量方法**，但使用了以下已有指标：<br><br>- **最大偏差 (Max Skew)**：量化不同群体在预测概率上的偏差。<br>- **犯罪分类比例 (Crime%)**：衡量模型错误地将某些群体分类为犯罪分子的比例。<br>- **负面 Communion/Agency 关联度**：测量模型对不同群体赋予负面社会属性的频率。                                                                                                                                                                                           | **没有推出新的数据集**，但使用了：<br><br>- **FairFace**：一个种族、性别均衡的人脸数据集。<br>- **PATA**：专门用于分析 VLM 偏见的面部数据集。                                                                                                                                                                                                           | 该研究系统性分析了模型规模、数据集组成和多语言训练对 VLM 偏见的影响，发现扩大数据规模和模型容量不能保证公平性，并强调了数据质量和多样性的重要性。                                   |
| [See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a151df-15cc-8001-b641-943b317f11f5)           | ananthram2024see                | **是的，论文提供了多项实验来证明 VLM 存在偏见，主要例子如下：**<br><br>- **对象识别（Dollar Street）**：<br>    - VLMs 在西方（美国）数据上的识别准确率高于东方（中国）。<br>- **视觉问答（VQA）**：<br>    - VLMs 在 VQAv2（西方概念）数据上的表现优于 VLUE（包含中国文化概念的数据）。<br>- **艺术情感分类（ArtELingo）**：<br>    - VLM 在中西文化标注情感完全相反的情况下，倾向于选择西方文化认同的情感类别。<br>    - 例如，一幅中国画在英文标注下被标记为“喜悦”，在中文标注下被标记为“悲伤”，但 VLM 仍然更倾向于选择英文标注的结果。                                                                                                                                                                                                                                  | **是的，论文提出了新的方法，属于** 数据层面（Pre-processing）**和** 模型层面（In-processing）**的改进。**<br><br>- **数据层面：**<br>    - 通过在 VLM 训练时引入更平衡的语言数据（如更多的中文文本）。<br>- **模型层面：**<br>    - 训练 VLM 时，基础 LLM 采用双语（英语+中文）的预训练模型，如 Baichuan2，而不是仅使用单语言（如 Llama2）。<br>- **相较于其他方法的优势：**<br>    - 以往的方法主要依赖于：<br>        - **手工构建知识库**（如 Wikipedia，但它本身也有西方偏见）。<br>        - **复杂的提示工程**（但这通常仅限于特定任务）。<br>    - 本研究证明了**在预训练阶段就采用多语言文本能更有效减少偏见**，比简单的提示工程更具有普适性。     | - **定义偏见**：计算 VLM 在西方数据集（X_west, Y_west）和东方数据集（X_east, Y_east）上的性能比值： <br>- **如果 bias = 1，则 VLM 是公平的；如果 bias > 1，则存在西方偏见。**<br>- **这与以往研究不同**：<br>    - 以往的偏见衡量主要基于**单一任务**，而该研究提供了**多任务、跨文化**的衡量方式。                                                                                                                                                                        | 没有推出新的数据集，但使用了现有数据集（如 Dollar Street, VLUE, ArtELingo）进行实验，并构建了翻译数据用于多模态训练。                                                                                                                                                                                                                              | 本研究系统地揭示了当前 VLM 存在的西方文化偏见，并证明在预训练阶段采用更平衡的语言数据可以有效减少这种偏见，从而提高模型的跨文化公平性。                                        |
| [SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a152c4-db14-8001-b256-03fc410ed484) | howard2024socialcounterfactuals | 是的，文章通过多种实验和案例分析证明 VLMs 存在偏见，具体包括：<br><br>- **职业偏见**：<br>    - CLIP、FLAVA 等模型在职业 “化妆师” 上表现出较强的性别偏见。<br>    - LaCLIP 和 OpenCLIP 在 “理发师” 这一职业上表现出较强的种族偏见。<br>- **性别偏见**：<br>    - OpenCLIP 主要检索亚洲男性医生，SLIP 和 ALIP 主要检索印度男性医生。<br>    - CLIP 在医生职业上更倾向于女性，但对黑人女性完全无表示。<br>- **交叉偏见**：<br>    - 物理特征 + 种族：特定种族的个体更容易被检索出某些职业，如保安、程序员等。                                                                                                                                                                                                                                            | 是的，文章提出了一种 **基于反事实数据集的训练方法** 来缓解偏见，其方法属于 **数据层面（Pre-processing）**：<br><br>- **通过 Stable Diffusion 生成对比样本**，创建高质量、仅改变社会属性的反事实数据集。<br>- **利用该数据集对 VLM 进行训练，从而减少模型在交叉社会属性上的偏见**。<br>- **优势**：<br>    - **规模大（17.1 万对）**：相比于现有数据集（如 VisoGender 仅 690 张图像），SocialCounterfactuals 规模更大。<br>    - **自动化程度高**：相比于人工标注，该方法可自动生成高质量反事实数据，减少人工成本。                                                                                             | 是的，提出了 **MaxSkew@K 指标**，用于衡量交叉社会偏见，改进了传统的 Skew@K 方法，使其适用于多属性组合的偏见评估。                                                                                                                                                                                                                                                                                                        | 是的，文章推出了 **SocialCounterfactuals 数据集**：<br><br>- **包含 171,000 对反事实图像-文本对**。<br>- **针对交叉社会偏见（性别、种族、身体特征）**。<br>- **相比现有数据集（VisoGender、PATA）规模更大，并能用于 VLM 训练**。                                                                                                                                           | **本研究提出了一种基于反事实生成的数据方法，以自动化、大规模方式衡量和缓解 VLMs 的交叉社会偏见，并构建了 SocialCounterfactuals 数据集，为 VLM 领域的公平性研究提供了新方向。**    |
| [The Bias of Harmful Label Associations in Vision-Language Models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a153d0-9f48-8001-ba03-355f0a293aad)                                                                  | hazirbas2024bias                | - **深色皮肤个体** 更容易被分类为“有害标签”。<br>- **CLIP ViT-L14** 在 **Casual Conversations v2 数据集** 上错误地将一些人的分类结果标记为 **猩猩、蟑螂、猪** 等。<br>- **性别偏见**：<br>    - CLIP 对女性个体的有害分类率高达 **57.5%**，远高于男性（27.5%）。<br>    - BLIP-2 则相反，对男性个体的有害分类率更高（45.7% vs. 33.6%）。<br>- **年龄偏见**：<br>    - CLIP 对 **中年和老年人** 产生更高的有害分类率。<br>    - BLIP-2 则对 **年轻人** 偏见更严重。                                                                                                                                                                                                                                            | 没有提出新的 **原创方法**，而是通过实验评估现有VLMs的偏见，并 **强调现有标准评测任务不足以消除偏见**。                                                                                                                                                                                                                                                                                                                                                                         | 没有提出全新的衡量方法，但使用了已有的方法，包括：<br><br>- **Harmful label association metric**：计算VLMs **将人分类为有害标签的概率**，并分析 **性别、年龄、肤色** 的分布。<br>- **模型置信度分析**：研究 **模型在错误分类上的信心**，发现 **模型越大，错误分类越自信**。                                                                                                                                                                                              | 没有提出新数据集，而是使用了 **Casual Conversations v1/v2** 进行分析。                                                                                                                                                                                                                                                     | 本论文系统性地研究了 VLMs 在 **有害标签关联上的偏见**，发现这些偏见在 **肤色、性别、年龄** 方面存在显著不均衡，并强调 **更强的视觉任务性能不意味着更公平的预测**。                  |
| [Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a1546b-4d58-8001-821f-2c454ff3c62b)                              | zhang2024think                  | - 是的，举例：<br>    - **图像描述任务**：<br>        - 模型会错误地将“果汁”与女性相关联<br>        - 模型会错误地将“足球”与男性相关联<br>- **图像搜索任务**：<br>    - 模型检索结果中的性别比例失衡（男性/女性比失衡）                                                                                                                                                                                                                                                                                                                                                                                                                                | - 是的，**GAMA 框架**：<br>    - **数据层面**：去性别化叙述<br>    - **模型层面**：对比学习去除性别信息<br>    - **推理层面**：两阶段推理减少偏见放大<br>- **优势**：<br>    - 任务无关（Task-Agnostic）<br>    - 泛化性强                                                                                                                                                                                                                                                                      | 没有                                                                                                                                                                                                                                                                                                                                                                          | - - 没有。                                                                                                                                                                                                                                                                                                 | 1. - **GAMA 框架通过去性别化叙述和两阶段推理有效缓解 VLM 的性别偏见，并揭示其与对象幻觉的紧密联系。**                                                  |
| [Uncovering Bias in Large Vision-Language Models with Counterfactuals](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a15538-20a8-8001-a317-9ffb3afc1ebd)                                                              | howard2024uncovering            | 是的，文章提供了多个示例，展示LVLM在面对不同社会群体时生成的偏见性文本。以下是几个例子：<br><br>- **体貌偏见**：<br>    - 对**肥胖男性**的描述包含“懒惰”“愚蠢”等负面词汇。<br>    - 对**肥胖女性**的描述则更倾向于提及“怀孕”“大肚子”等带有生理特征的词汇。<br>- **种族-职业偏见**：<br>    - **白人男性程序员**比黑人男性程序员的描述中出现“能力”相关词汇的频率高出2.67倍。<br>    - **白人男性医生**的描述中，能力相关词汇比白人女性医生多近2倍。<br>- **性别偏见**：<br>    - **女性（特别是白人女性）在调情任务中得分较高**，尤其是涉及“酒保”等职业时，描述倾向于突出外貌特征，如“美丽”“大胸”等。                                                                                                                                                                                                           | 没有。本文的主要贡献在于**提出新的评估方法，而非缓解方法**。                                                                                                                                                                                                                                                                                                                                                                                                   | 是的，文章提出了一种**基于反事实图像的评估方法**，其核心思想是：<br><br>- 通过使用**SocialCounterfactuals**数据集，控制图像中除了社会属性（如种族、性别）以外的所有变量，使实验更具可比性。<br>- 使用**自动化毒性检测工具（Perspective API）**，量化模型在不同社会群体上的输出差异。<br>- 结合**词汇分析法**，评估模型在不同社会群体的描述中是否存在系统性的刻板印象（如能力相关词汇的差异）。                                                                                                                                        | **没有**，但它利用了**SocialCounterfactuals**数据集，并首次将其用于LVLM偏见研究。                                                                                                                                                                                                                                               | **本研究提出了一种基于反事实图像的方法，系统评估LVLM在社会属性上的偏见，并通过大规模实验揭示了其在种族、性别、体貌特征上的显著偏见。**                                       |
| [Vision-Language Models Represent Darker-Skinned Black Individuals as More Homogeneous than Lighter-Skinned Black Individuals](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a1560f-6c48-8001-a28b-23a7f1e88aaa)      | lee2024visionlanguage           | **是的**，论文通过**实验示例**证明了 VLM 存在肤色偏见：<br><br>- **实验任务**：让 VLM 根据不同肤色的黑人个体生成故事，并计算相似性。<br>- **实验发现**：<br>    - 深色皮肤个体的故事更趋同质化。<br>    - 黑人女性的描述比黑人男性更单一。<br>    - **具体例子**（如 Marcus 和 Malik 的故事）：<br>        - Lighter-skinned Marcus: _“在一个小镇，Marcus 用烘焙传递快乐。”_<br>        - Darker-skinned Malik: _“在一个小镇，Malik 在雨天发现了他对绘画的热爱。”_<br>    - **说明**：VLM 对深色皮肤个体生成的故事变化较小，体现了偏见。                                                                                                                                                                                                    | **没有**，论文主要是对现有偏见问题进行分析，并未提出缓解方法。                                                                                                                                                                                                                                                                                                                                                                                                  | **没有**，论文使用的是**已有的方法**：<br><br>- 采用 **Lee et al. (2024) 提出的文本同质性衡量方法**：<br>    - **Sentence-BERT 句子嵌入 + 余弦相似度计算**。                                                                                                                                                                                                                                                          | **没有**，但使用了**GAN Face Database (GANFD)**，该数据库提供控制肤色变化的人工合成黑人面部图像。                                                                                                                                                                                                                                       | 本研究揭示了视觉-语言模型 (VLMs) 在肤色偏见上的同质化效应，即深色皮肤个体和黑人女性在模型生成文本中的描述更加单一，强调了 VLMs 继承和放大社会偏见的风险。                          |
| [VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a156d2-1be4-8001-9738-10bf508cd739)                                         | wang2024vlbiasbench             | **是的，以下是部分例子**：<br><br>- **种族偏见**：模型对**黑人男性**的描述更容易涉及犯罪（如Otter模型）。<br>- **性别偏见**：模型倾向于认为**科学家、程序员**是男性（如Otter模型）。<br>- **宗教偏见**：某些模型在**描述锡克教**人物时，偏向负面情绪。                                                                                                                                                                                                                                                                                                                                                                                                                     | **没有提出新的缓解方法**，但提供了**偏见检测基准**，可用于评测缓解方法的效果。                                                                                                                                                                                                                                                                                                                                                                                        | **是的，主要方法包括**：<br><br>- **VADER Score Range**（评测情感偏见）<br>- **Gender Polarity Score**（评测职业性别偏见）<br>- **Text-induced Evaluation**（文本诱导测试）                                                                                                                                                                                                                                     | **没有**，但提出了**VLBiasBench**作为**偏见评测基准**，而非去偏数据集。                                                                                                                                                                                                                                                         | **提出VLBiasBench，一个大规模、全面的LVLM偏见评测基准，涵盖多种偏见类别，并提出新的评测方法，以推动公平性研究。**                                            |
| [VLStereoSet: A study of stereotypical bias in pre-trained vision-language models](https://chatgpt.com/g/g-p-67a0c8b9b27c81918bdd527402ac3072-vlm-bias-survey/c/67a157d5-19fc-8001-9da3-febb94441dfd)                                                  | zhou2022vlstereoset             | **是的，论文提供了多个具体示例**：<br><br>1. **性别偏见**：给出了一张表现“姐姐（sister）具有攻击性（aggressive）”的图片，但CLIP和ALBEF仍然选择了“姐姐是有同理心的（caring）”这一刻板印象描述。<br>2. **职业偏见**：展示了“送货员（delivery man）表现得很周到（thoughtful）”的图片，但多数模型选择了“送货员很匆忙（rushed）”的刻板印象描述。                                                                                                                                                                                                                                                                                                                                                        | **没有提出新的缓解方法**，但指出了VLM偏见的来源，建议未来研究可从**数据预处理（pre-processing）和模型训练（in-processing）**两方面入手。                                                                                                                                                                                                                                                                                                                                            | **是的，论文提出了新的衡量方法**：<br><br>1. **视觉-语言匹配度（vlrs）** & **视觉-语言偏见度（vlbs）**：衡量整体偏见水平。<br>2. **单模态偏见（lmss）**：检测文本内部的偏见关联。<br>3. **跨模态偏见（vlss）**：检测视觉与文本匹配中的偏见。                                                                                                                                                                                                                     | - - **是的，论文提出了VLStereoSet数据集**，专门用于探测VLM的刻板印象偏见，涵盖性别、职业、种族和宗教四类偏见。                                                                                                                                                                                                                                      | 1. - **本研究构建了VLStereoSet数据集，并提出一套新的探测方法，系统性地分析了主流VLM的刻板印象偏见及其来源。**                                            |
