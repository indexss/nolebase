# Bias术语

sensitive protected attributes 敏感保护属性（如种族、性别、年龄或族裔等人口统计变量）

---

- **Inherent correlation（内在相关 / 内在依赖）：** 指的是某些属性之间原本就存在实际、必然或合理的关联。例如，年龄与医疗需求、车辆里程与油耗，这些往往有某种“真实”因果或强相关。如果一个模型在预测中利用了这类“固有”相关性（只要符合伦理和法律要求），就不一定是偏见。
- **Spurious correlation（虚假关联 / 虚假依赖）：** 指的是属性与目标之间并不存在真正的因果或合理关联，但在训练数据中却呈现出了强相关。由于模型只要能降低损失，就会把这些“看似有用”的关联也学进去，最终在实际应用中导致错误或歧视性的决策。例如：
    - **背景偏见：** 在一个动物识别数据集中，如果所有骆驼都拍摄在沙漠背景，而马都拍摄在草地背景，模型可能把“沙漠”当作判断“骆驼”的关键特征，遇到背景非沙漠的骆驼就无法识别。
    - **人群偏见：** 如果数据集中黑人女性样本较少，模型可能“误学”到只有白人女性才对应某些职务或表情，从而在黑人女性身上预测失准。
    - 上述“背景 / 人群”信息与目标看似相关，但**实则只是数据收集过程中的巧合或分布不均匀导致**，这就属于“虚假关联”。

---
**用于纠正偏见的方法**称为**偏见缓解技术（mitigation techniques）**，这些方法也常被称为**去偏见技术（debiasing techniques）**。

---

计算机视觉模型的偏见可以出现在**不同的层面**，例如：

1. **数据集偏见（Dataset Bias）**
    
    - **问题**：如果一个模型**仅在某个特定数据集上训练**，它可能**无法泛化到其他数据集**。
    - **示例**：一个在人脸数据集 CelebA 训练的性别分类模型，可能在 RFW 这样的种族均衡数据集上表现较差。
    - **解决方案**：在**多个数据集上混合训练**，确保模型在不同的测试集上都能表现良好。
2. **表示偏见（Representation Bias）**
    
    - **问题**：有些数据集提供了**可以轻易区分类别的线索**，导致模型仅依赖这些**非核心因素**来做出预测。
    - **示例**：在**动作识别**任务中，如果训练数据集中“烹饪”场景大多出现在厨房，而“跑步”场景大多出现在操场，模型可能**仅依赖背景信息**就能进行分类，而不是学习真正的动作特征。
    - **解决方案**：使用**新的数据集**，要求模型必须**识别长期的运动模式**（而不是简单依赖背景信息）。

---
