# Fairness and Bias in Artificial Intelligence
## Introduction


## Research Questions
1. CV领域bias的表现和来源
    CV系统的公平性已经引起了广泛的注意。已经有很多研究从问题角度出发讨论了CV领域的bias，比如种族偏见，性别偏见以及光照适应性等。随着这些问题被发现，CV系统的bias已经不是孤立的问题，而是系统性问题。考虑到目前大部分的CV系统已经转向Learning based，其本质是从数据中学习模式，所以我们将从数据与模型两个角度出发，分析其表现和来源。
    [CV领域bias表现和来源](CV领域bias表现和来源.md)
  1. 简单谈谈bias定义
  2. 来自于数据的bias
  	1. 社会本身就有的bias被收录入了数据集
  	2. 数据采集过程中的缺陷
  	3. 人口学偏差和非人口学
  3. 来自于模型本身缺陷的bias
  	1. 学习过程中学习到了伪相关和固有相关
  	2. 模型设计与训练策略的放大或者弱化
  	3. 人口学与非人口学
1. CV领域de-bias的算法以及其他de-bias的努力
  1. 之前
  	1. 提到一些公平数据集
  	2. 数据去偏
  	3. 数据增强
  2. 之中
  	1. 对抗表示学习
  	2. 正则化/不变性学习
  3. 之后
  	1. 决策阈值或分数校准，后处理层面减轻群体差异


## Research Questions
1. CV领域bias的表现和来源
   - Bias的定义
   - Bias的分类
     - 人口学偏差（Demographic Bias）
     - 非人口学偏差（Non-Demographic Bias）
   - 数据来源的bias
     - 社会固有bias被引入数据集
     - 数据采集过程的问题
     - 任务设计或标注者的主观性
   - 模型本身的bias
     - 伪相关性与固有相关性
     - 训练策略与架构导致的偏差
     - 参数分布对不同群体的影响

2. CV领域de-bias的算法以及其他de-bias的努力
   - 数据层面的去偏（Pre-processing）
     - 公平数据集
     - 数据去偏（Re-sampling, Re-weighting）
     - 数据增强（GAN生成补充样本、多样性数据增强）
   - 模型层面的去偏（In-processing）
     - 对抗表示学习（Adversarial Representation Learning）
     - 正则化/不变性学习
   - 推理层面的去偏（Post-processing）
     - 决策阈值调整
     - 后处理校准

3. 公平性评测标准（Evaluation Metrics）
	[CV领域公平的定义](../CV领域公平的定义.md)
   - 统计公平性（Statistical Parity）
   - 机会公平性（Equalized Odds, Equal Opportunity）
   - Worst-group Accuracy


1. VLM领域bias的表现和来源
	-  Bias的定义，分类
     - 人口学偏差（Demographic Bias）
     - 非人口学偏差（Non-Demographic Bias）
   - 数据来源的bias
     - 社会固有bias被引入数据集
     - 数据采集过程的问题
     - 任务设计或标注者的主观性
   - 模型本身的bias
     - 伪相关性与固有相关性
     - 训练策略与架构导致的偏差
     - 参数分布对不同群体的影响

## Search Strategy
本综述对计算机视觉领域的偏见和公平性研究进行调查。我在多个文献数据库中搜索了相关文献，包括ACM Digital Library, Google Scholar以及arXiv。在检索过程中，我使用了两组并列的关键字，第一组搜索词为：computer vision, CV, image recognition, object detection, semantic segmentation, pose estimation。这些关键词涵盖了计算机视觉的主要任务和方向，确保了搜索结果集中于这一领域。第二组搜索词为：bias, fairness, discrimination, dataset bias, model bias, algorithmic discrimination。这些关键词用于筛选涉及偏见，公平性的研究。

接下来，我对筛选出的文献进行了人工筛选，排除了发布超过5年的文献以及仅仅泛泛而谈并未明确涉及计算机视觉的文献。最终筛选出的工作涵盖了计算机视觉领域的核心偏见问题，包括但不限于计算机视觉领域的数据集偏见，模型偏见，公平性评估指标以及去偏见方法。这一筛选策略确保了研究的广泛性和针对性，同时避免了与其他机器学习领域的公平性研究混淆。

本综述对计算机视觉领域的偏见和公平性研究进行调查。我在多个文献数据库中搜索了相关文献，包括 ACM Digital Library、Google Scholar 以及 arXiv。在检索过程中，我使用了两组并列的关键字，第一组搜索词为：computer vision, CV, image recognition, object detection, semantic segmentation, pose estimation。这些关键词涵盖了计算机视觉的主要任务和方向，确保了搜索结果集中于这一领域。第二组搜索词为：bias, fairness, discrimination, dataset bias, model bias, algorithmic discrimination。这些关键词用于筛选涉及偏见、公平性的研究。 

在筛选过程中，我主要关注近5年内（2019-2024）发表的文献，以保证研究的时效性。同时，我排除了泛泛讨论人工智能公平性但未具体涉及计算机视觉的文献。然而，为了提供完整的背景信息，本综述在介绍计算机视觉偏见的定义、早期研究和经典理论时，也适当引用了5年以上的奠基文献（如最早提出计算机视觉领域偏见的研究）。这一策略确保了研究的广泛性和针对性，同时避免了与其他机器学习领域的公平性研究混淆。

## Summary of Research
## Conclusions
## References




## 一、可研究的研究问题（Research Questions）

1. **RQ1：视觉-语言预训练模型中最常见的社会偏见类型有哪些？它们在不同数据集或应用场景（如图文检索、图像生成、视觉问答）中如何表现？**
   
    - **动机**：了解在 VLM 中出现的性别/种族/年龄等偏见具体体现形式，并梳理这些偏见在不同下游任务中的影响。
    - **意义**：帮助读者快速掌握主要的“痛点”与常见失误模式；为后续提出有针对性的缓解方法奠定基础。
2. **RQ2：目前主流的偏见测量及缓解方法（例如对抗训练、数据增广或后处理技术）在 VLM 中的有效性和局限性是什么？**
   
    - **动机**：概览现有去偏技术在视觉-语言多模态场景下的研究现状、评估指标与结果。
    - **意义**：识别已有方法的优点和不足，为后续改进提供思路，并帮助实践者更好地挑选与应用合适的技术。
3. （可选）**RQ3：在面对多种敏感属性（性别、种族、年龄、肤色等）并存的真实场景时，如何对 VLM 进行多属性联合去偏，以在不显著损害主任务性能的前提下最大化公平性？**
   
    - **动机**：真实应用通常涉及多种敏感属性的同时存在，单一属性去偏不足以应对复杂场景。
    - **意义**：为构建更全面、更具实用性的多模态公平解决方案指明方向，并揭示多属性间可能的交互效应。

可根据实际文献数量、研究深度，选取 1-3 个问题作为论文聚焦点，或者将 RQ2 与 RQ3 合并。

---

## 二、写作大纲（Paper Outline）

以下大纲结合了作业要求所规定的必备章节，并在“文献综述（Summary of Research）”部分细分出与“偏见挑战”及“缓解技术”相关的几个关键小节，供参考。

### 1. Introduction

1. **研究背景**
    - 简要介绍 AI 系统中存在的社会偏见问题，以及为何公平性在智能交互系统（或人工智能）中重要。
    - 引出视觉-语言模型（VLM）在多模态领域的快速发展，以及其潜在的偏见与风险。
2. **研究目的和意义**
    - 本文主要围绕视觉-语言模型的公平与偏见展开系统文献回顾。
    - 指明该主题对于学术界与工业界的重要性。

### 2. Research Questions

- 列出本文主要回答的研究问题（如上给出的 RQ1、RQ2、RQ3）。
- 简要说明这些问题的动机及文章将如何尝试回答。

### 3. Search Strategy

1. **检索数据库与平台**
    - 指定使用的数据库（如 ACM Digital Library 等），并说明选择原因。
2. **检索关键词**
    - 示例：("Vision-Language Models" OR "Multimodal") AND ("bias" OR "fairness" OR "social bias")。
3. **时间范围**
    - 默认回顾近 3 年（例如 2020 年至今），若文献不足可扩展至近 5 年。
4. **筛选标准**
    - 说明排除重复、与主题相关度低等文献的方式；最终保留多少篇文献。

### 4. Summary of Research

本节是整篇文献综述的核心，可按如下子部分组织：

#### 4.1 偏见与公平性概念在 VLM 场景下的特殊性

- 简述为什么视觉-语言模型相较于纯语言模型或纯视觉模型更易（或更难）产生偏见。
- 结合已有研究，讨论多模态数据的标注偏差、训练流程中语言先验与视觉先验的冲突等问题。

#### 4.2 偏见的主要类型和表现形式

- 汇总文献中报道的典型社会偏见（如性别刻板印象、种族标签错误、非人化标签等）以及在实际应用/数据集上的具体事例。
- 可能引用：CLIP、BLIP、Stable Diffusion 等模型中出现的偏见现象案例。

#### 4.3 偏见的测量与评估方法

- 内在测量：如 Grounded-WEAT、iEAT，以及多模态空间中的嵌入分析。
- 外在测量：零样本分类错误率差异、图文检索中的人口统计分布偏差、文本-图像生成中的性别/种族歧视等。
- 对比各测评指标的优缺点及适用范围。

#### 4.4 偏见缓解技术

- **数据级方法**：对图文对齐数据进行增强或清洗；对不平衡敏感属性进行重新采样。
- **模型级方法**：对抗训练、多任务正则、视觉/文本特征解耦等。
- **推断级方法**：向量投影去除敏感特征、文本提示后处理、额外微调模块减少偏见。
- 结合已有实证，讨论这些方法在多模态场景下的优势和不足。

#### 4.5 多属性多模态场景中的公平性挑战

- 扩展讨论同时涉及性别、种族、年龄、宗教等多种敏感属性时可能的复杂性。
- 提及现有对多属性联合去偏研究的缺失与潜在方向。

### 5. Conclusions

1. **主要发现**
    - 结合前文文献讨论，简要回答研究问题 RQ1、RQ2（及 RQ3）。
2. **研究局限与未来展望**
    - 现有研究在哪些方面仍显不足（数据规模、方法通用性、跨文化适用性等），以及后续可能的改进方向。
    - 强调进一步研究对真实世界部署和多元群体影响的重要性。

### 6. References

- 按指定的参考文献格式列出所有引用过的论文、报告等。
- 可以使用 BibTeX 或其他文献管理工具来自动生成参考文献列表。

许多预训练VLMs（如CLIP）在训练之初仅作为Encoder，用来将多模态的数据编码到同一个向量空间，学习跨模态对齐。而预训练的VLMs可以用于多种下游任务。通常，zero-shot文字-图片检索属于VLMs本身的任务，而图像字幕生成，文本图像生成，视觉问答，视觉推理等属于下游任务。我们将介绍偏见在VLMs本身以及其下游任务中的体现，并介绍其来源。

许多预训练VLMs（如 CLIP）在训练初期主要作为都是Encoder，用于将多模态数据映射到同一向量空间，以学习跨模态对齐。预训练的 VLMs 可广泛应用于多种下游任务。通常，文本-图像检索被视为模型的固有能力，而图像字幕生成、文本-图像生成、视觉问答、视觉推理等则属于下游任务。本文将探讨偏见在 VLMs 本身及其下游任务中的体现，并分析其来源。

许多早期的预训练 VLMs（如 CLIP）采用了Encoder-only的架构，用于将多模态数据映射到同一向量空间，以学习跨模态对齐。而随着多模态任务的拓展，越来越多的 VLMs将这种思路用于下游任务中，将

VLMs在多种下游任务中都存在着偏见。对于Zero-shot图像文本检索任务，zhang2024joint发现，CLIP普遍将职业表述与男性嵌入距离更近，离女性更远。hamidieh2024identifying发现，检索恐怖分子，中东男性出现的频率远高其他；检索家庭主妇，经常出现印度女性。在图像字幕生成任务中，jung2024unified发现运动员经常被描述为男性，zhang2024think发现模型会错误地将“果汁”与女性相关联，将“足球”与男性相关联。在文本图像生成任务中，jung2024unified发现即使使用中性提示词，程序员也会被生成为男性，sathe2024unified发现DALL-E-3模型在绘制职业角色中，科学家多为男性，护士多为女性。在视觉问答任务中，wolfe2022american发现96.7%的白人被认为是美国人，而只有2.8%亚裔被认为是美国人，即使其真实标签为美国人。在视觉推理任务中，abdollahi2024gabinsight发现**如果画面只有女性**，模型能够正确识别其正在修理设备的活动并生成正确的描述。**如果画面中同时有男性和女性**，即使女性正在修理设备，模型仍然倾向于认为**男性是执行者**。lee2024visionlanguage发现提供个体的照片并生成故事，深色皮肤个体的故事更趋同质化。这些都说明着VLMs在多种下游任务中都存在着偏见。

VLMs中偏见的来源可以分成两类，一种是继承自训练数据的bias，一种是VLMs本身模型的问题。
对于继承自数据的bias，VLMs训练数据中的偏见很难消除。lee2023surveya由于文本可描述更多抽象特征，而图像只提供可见信息，所以跨模态对照以及构造对抗样本的难度大，这增加了模型从数据中继承bias的可能性。此外，数据集的标签也存在着更多bias，这是由于VLMs需要大规模数据集训练，而通常这类数据集有着长尾分布，即大多数标签只与少量样本关联。另外，由于难以在图像数据集中准确消除敏感受保护属性，以及风格，背景等导致的虚假相关性难以消除。

VLMs 中的偏见主要来源于两方面：一是继承自训练数据的偏见（bias），二是模型自身的结构与学习机制导致的偏差。

对于源自训练数据的偏见，由于 VLMs 依赖大规模数据集进行训练，这些数据中固有的偏见难以完全消除。lee2023surveya 指出，由于文本能够描述更多抽象特征，而图像仅提供可见信息，因此跨模态对比学习与对抗样本构造的难度较高，从而增加了模型从数据中继承偏见的可能性。此外，VLMs 训练数据集的标签往往也存在偏见。这是由于大规模数据集通常呈现长尾分布，即大多数标签仅与少量样本关联，导致模型对某些类别的学习不均衡。此外，在图像数据集中，难以准确去除敏感受保护属性，同时，风格、背景等因素可能引入虚假相关性，使得偏见进一步难以消除。

对于模型自身的问题，weng2024images研究结果显示在VLM中图像特征对偏见的贡献显著高于文本特征，在MSCOCO和PASCAL-SENTENCE数据集中分别占32.57%和12.63%的偏见。这突显了视觉和文本模态的结构集成如何不同地影响模型内的偏见。Scaling-Law已成为了基本的共识，即更大的模型效果更好，但hazirbas2024bias等人发现，**更大的 Transformer 模型（如 CLIP ViT-L14）在有害预测上更自信**，即使预测是错误的，这也加剧了bias的出现。

不同的VLMs所展示的偏见偏向并不相同，在不同的任务上也不相同，对于模型是否存在偏见不可一概而论。一些VLMs评估体系揭露了这一事实。hamidieh2024identifying提出了So-B-IT分类体系，将偏见分为了十种类型并使用了C-ASC指标计算了四种CLIP变体在每个类别的偏见程度。结果表明，OpenCLIP在职业偏见上最严重，而虽然DebiasCLIP成功减少了性别偏见，但加剧了种族偏见。sathe2024unified提出了一个偏见评估框架以及一个新的偏见度量指标Neutrality，对多种VLMs在多个任务上进行了衡量，结果显示，尽管都存在着性别bias，LLaVA和CoDi更倾向男性，而ViPLLaVA更倾向于女性。尽管都存在种族bias，DALL-E-3生成的职业图像主要是白人，但CoDi更偏向非裔。这也说明了不同模型的偏见偏好不同。另外，wang2024vlbiasbench提出了**VLBiasBench**，一个用于评测VLMs的综合性基准，涵盖了9种独立偏见以及两种交叉偏见。结果显示，**InstructBlip-flan-t5-xl** 在**种族偏见（Race Bias）**上偏见最大（排名第一），但在**职业偏见**上表现较差，Blip2-flan-t5-xl** 在**宗教偏见（Religion Bias）**上偏见最严重（排名第一），但在**种族偏见**上的排名相对靠后。


不同的 VLMs 在偏见的类型及程度上表现出差异，并且在不同任务中的偏见倾向亦有所不同。因此，对于某一特定模型是否存在偏见，不能一概而论。一些 VLMs 评估体系揭示了这一现象的复杂性。
Hamidieh2024identifying 提出了 So-B-IT 分类体系，将偏见划分为十个类别，并使用 C-ASC 指标量化四种 CLIP 变体在各类别上的偏见程度。研究结果表明，OpenCLIP 在职业偏见方面最为严重，而尽管 DebiasCLIP 在一定程度上成功降低了性别偏见，却同时加剧了种族偏见。这揭露了模型对偏见的倾向并不相同。
Sathe2024unified 提出了一个偏见评估框架，并引入了新的偏见度量指标 Neutrality，对多种 VLMs 在多个任务上的表现进行了衡量。实验结果显示，尽管所有模型均存在性别偏见，LLaVA 和 CoDi 更倾向于男性，而 ViPLLaVA 则更倾向于女性。此外，尽管不同模型均表现出种族偏见，DALL-E-3 生成的职业图像主要为白人，而 CoDi 更倾向于生成非裔个体。这一发现进一步表明，不同模型在偏见倾向上存在显著差异。
此外，Wang2024vlbiasbench 提出了 **VLBiasBench**，一个用于评估 VLMs 偏见的综合性基准，涵盖 9 种独立偏见及 2 种交叉偏见。研究结果表明，**InstructBlip-flan-t5-xl** 在 **Race Bias** 方面的偏见排名第一，但在 **职业偏见** 方面的表现相对较弱。而 **Blip2-flan-t5-xl** 在 **宗教偏见** 上的偏见排名第一，但在 **种族偏见** 方面的排名相对靠后。

de-bias的方法通常有三个对象，即对修改训练数据以消除偏见，修改模型架构或训练过程以消除偏见，修改推理过程以消除偏见。我将会介绍三种方法。

对于修改训练数据消除偏见，howard2024socialcounterfactuals通过 Stable Diffusion生成对比样本，构建了大规模的SocialCounterfactuals数据集，其拥有17.7万对数据，从而减少模型的偏见。
zhang2022counterfactually通过对抗攻击在数据阶段减少偏见，对于图像模态，通过对抗攻击修改图像中的性别特征但保留其他信息，对于文字模态，直接替换其中的性别相关词汇生成反事实样本，从而减少偏见。
ananthram2024see发现通过在VLM的训练数据中加入更平衡的语言数据（如更多的中文文本）就可以有效减少与训练模型的偏见，比提示工程更有普适性。

对于模型层面的方法，seth2023dear提出了DeAR训练对抗性残差网络，仅需要训练一个小型残差快添加到视觉编码器的图像表示上，无需重新训练整个VLM就可降低偏见。而jung2024unified对其方法进行了改进，仅需几十秒训练一个随机森林分类起计算敏感特征，无需重新训练，就可以直接在冻结的VLM参数上使用，节省了计算资源，也可降低偏见。另外，weng2024images利用了MTCNN面部识别以及MobileNet进行性别分配识别替换人脸区域，模糊性别特征，并通过替换性别词并计算其反响性别词的特征均值，减少文本的性别信息影响，有效减少了偏见

对于推理层面的方法，chuang2023debiasing提出了一种基于文本嵌入投影的方法，通过提示词测量偏见并通过正交投影去偏。他们证明了校准的投影矩阵可以等价于一个均衡化损失，确保模型不会偏向特定群体。janghorbani2023multimodal通过计算模型嵌入的不同维度对偏见的贡献，选取了对偏见贡献最大的N个维度，评估其对结果的影响，最后移除影响最大的冗余维度，减少偏差，仅需处理嵌入空间，不影响推理速度，偏见减少高达93%，准确率仅下降1.1%（MMBias 数据集）和 1.3%（CIFAR-100 数据集）。


本 Survey 深入探讨了 VLMs 的偏见与公平性问题，通过系统搜索搜集相关文献，并系统分析了偏见在 VLMs 中的具体表现、来源，以及不同 VLMs 在不同任务和模态下的偏见倾向。此外，本文还总结了当前主要的去偏方法，包括数据层面的反事实样本生成、模型结构优化、以及推理阶段的偏见消除策略。希望能为未来VLMs去除偏见的研究提供帮助
