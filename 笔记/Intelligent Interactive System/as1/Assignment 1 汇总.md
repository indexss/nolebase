# Assignment 1 信息汇总

## 生物信息中的bias
1. 数据库层面：欧洲人的数据多于其他人种

## HCI中的bias
1. 图形化界面是不是对视障人士的Bias？
   视障人士有那么多，但是因为他们本身没有那么高的消费能力
   我们的科技宁愿去迎合一小撮某个特定领域的人群，也不去帮助视障人群
   blind and low vision
   这一论断是综合性的，具体就是整个互联网无障碍的问题，包括当前的手机界面是不是对于老年人来说
   太新太复杂了，一些工作会把现代的界面映射到遥控器上，遥控器或许是老年人更熟悉的载体

2. text 2 image，好像positive一点的词更容易出现白人形象？然后什么棉花地之类的就是黑人
   以前表情包也只有白人的表情包现在可以选肤色了

3. 比如翻译功能那么高级，在一些南方国家中他的语音识别率会不会比较低？

4. 非母语者在使用某些网站的时候有没有障碍？

5. 互联网上的搜索结果是否是穆斯林友好的？ 


## 各种bias的分类
在人工智能（AI）系统中，偏见（Bias）可能出现在多个环节，影响模型的公平性和可靠性。以下是 **Data Bias**、**Algorithmic Bias**、**Interpretation Bias** 的详细解释，以及其他常见偏见类型的补充：

---

### **1. Data Bias（数据偏见）**
- **定义**：训练数据中存在的系统性偏差，导致模型无法公平反映真实世界。
- **成因**：
  - **样本偏差**：数据集中某些群体或现象代表性不足（如人脸识别数据中缺乏深肤色样本）。
  - **历史偏见**：数据反映历史歧视（如招聘数据中性别比例失衡）。
  - **标注偏差**：人工标注者的主观倾向（如对“攻击性言论”的标注标准不一致）。
- **案例**：  
  ProPublica 发现 COMPAS 再犯预测模型因训练数据中黑人被告比例过高，导致对黑人群体误判率更高。

---

### **2. Algorithmic Bias（算法偏见）**
- **定义**：模型在训练或推理过程中因算法设计引入的偏差。
- **成因**：
  - **目标函数偏差**：优化目标忽视公平性（如仅追求整体准确率，忽略少数群体表现）。
  - **特征选择偏差**：模型过度依赖与敏感属性（种族、性别）相关的代理变量（如邮编关联种族）。
  - **模型结构偏差**：某些架构对特定数据分布敏感（如RNN对长文本中的顺序偏见）。
- **案例**：  
  Amazon 招聘算法因历史数据中男性工程师占主导，导致算法对女性简历降权。

---

### **3. Interpretation Bias（解释偏见）**
- **定义**：人类在解释模型输出时引入的主观偏差。
- **成因**：
  - **因果误判**：将相关性解释为因果关系（如将“邮政编码”误认为犯罪原因）。
  - **特征重要性误读**：高估某些特征的影响（如认为“收入”是贷款审批的唯一因素）。
  - **可视化误导**：解释工具（如LIME、SHAP）的可视化结果被选择性呈现。
- **案例**：  
  医疗诊断模型中，“患者年龄”可能被错误解读为直接风险因素，而忽略其与真实病因（如慢性病）的间接关联。

---
以下是导致算法偏见（Algorithmic Bias）的主要原因的系统性总结，采用中英对照格式并辅以分类说明：

---

### **算法偏见的成因分类**
#### **1. 数据固有偏见**  
**Data Inherent Bias**  
- **定义**：训练数据本身包含历史或系统性偏见  
- **来源**：  
  - 设备测量偏差（如血氧仪对深肤色人群的误差）  
  - 人类历史决策偏见（如司法系统中的种族歧视记录）  
  - 错误标注或虚假数据（如社交媒体中的仇恨言论误标为中性）  
- **影响**：算法放大社会既有不平等（例：招聘算法延续性别职业刻板印象）

#### **2. 数据缺失偏见**  
**Missing Data Bias**  
- **类型**：  
  - **结构性缺失**：特定群体数据未被收集（如医疗研究中缺乏少数族裔基因组数据）  
  - **选择偏差**：采样方法导致覆盖不全（如仅通过医院就诊记录获取健康数据，忽略未就医人群）  
- **影响**：模型对缺失群体预测失效（例：糖尿病预测模型在低收入社区准确率下降30%）

#### **3. 算法目标偏差**  
**Objective-Induced Bias**  
- **机制**：  
  - 损失函数仅优化整体准确率（例：人脸识别在亚裔群体中错误率升高）  
  - 评估指标忽视分布公平性（如仅用AUC忽略不同亚组的F1-score差异）  
- **典型场景**：  
  - 资源分配模型（如ICU床位分配优先年轻患者）  
  - 信用评分系统（少数族裔贷款拒绝率异常高）

#### **4. 代理属性偏差**  
**Proxy Attribute Bias**  
- **运作方式**：  
  | 敏感属性 | 常见代理属性 |  
  |----------|--------------|  
  | 种族     | 邮编、消费习惯 |  
  | 性别     | 购物记录、职业 |  
  | 年龄     | 设备使用时长 |  
- **隐蔽性**：  
  - 通过特征相关性间接传递歧视（例：美国医保算法使用医疗支出作为健康需求代理，低估黑人患者真实需求）  
- **检测难点**：需因果推断技术（如反事实分析）识别